{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A unified platform to manage high-throughput workflows across the HPC landscape.</p> <ul> <li>Balsam Documentation (branch: main)</li> <li>Legacy Balsam Documentation (branch: master)</li> </ul> <p>Run Balsam on any laptop, cluster, or supercomputer.</p> <pre><code>$ pip install --pre balsam\n$ balsam login\n$ balsam site init my-site\n</code></pre> <p></p> <p>Python class-based declaration of Apps and execution lifecycles.</p> <pre><code>from balsam.api import ApplicationDefinition\n\nclass Hello(ApplicationDefinition):\n    site = \"my-laptop\"\n    command_template = \"echo hello {{ name }}\"\n\n    def handle_timeout(self):\n        self.job.state = \"RESTART_READY\"\n</code></pre> <p>Seamless remote job management.</p> <pre><code># On any machine with internet access...\nfrom balsam.api import Job, BatchJob\n\n# Create Jobs:\njob = Job.objects.create(\n    site_name=\"my-laptop\",\n    app_id=\"Hello\",\n    workdir=\"test/say-hello\",\n    parameters={\"name\": \"world!\"},\n)\n\n# Or allocate resources:\nBatchJob.objects.create(\n    site_id=job.site_id,\n    num_nodes=1,\n    wall_time_min=10,\n    job_mode=\"serial\",\n    project=\"local\",\n    queue=\"local\",\n)\n</code></pre> <p>Dispatch Python Apps across heterogeneous resources from a single session.</p> <pre><code>import numpy as np\n\nclass MyApp(ApplicationDefinition):\n    site = \"theta-gpu\"\n\n    def run(self, vec):\n        from mpi4py import MPI\n        rank = MPI.COMM_WORLD.Get_rank()\n        print(\"Hello from rank\", rank)\n        return np.linalg.norm(vec)\n\njobs = [\n    MyApp.submit(\n        workdir=f\"test/{i}\", \n        vec=np.random.rand(3), \n        ranks_per_node=4,\n        gpus_per_rank=0,\n    )\n    for i in range(10)\n]\n\nfor job in Job.objects.as_completed(jobs):\n   print(job.workdir, job.result())\n</code></pre>"},{"location":"#features","title":"Features","text":"<ul> <li>Easy <code>pip</code> installation runs out-of-the-box on several HPC systems and is easily adaptable to others.</li> <li>Balsam Sites are remotely  controlled by design: submit and monitor workflows from anywhere</li> <li>Run any existing application, with flexible execution environments and job lifecycle hooks</li> <li>High-throughput and fault-tolerant task execution on diverse resources</li> <li>Define data dependencies for any task: Balsam orchestrates the necessary data transfers</li> <li>Elastic queueing: auto-scale resources to the workload size</li> <li>Monitoring APIs: query recent task failures, node utilization, or throughput</li> </ul>"},{"location":"LICENSE/","title":"License","text":"<p>BSD 3-Clause License</p> <p>Copyright (c) 2021, UChicago Argonne LLC All rights reserved.</p> <p>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:     * Redistributions of source code must retain the above copyright       notice, this list of conditions and the following disclaimer.     * Redistributions in binary form must reproduce the above copyright       notice, this list of conditions and the following disclaimer in the       documentation and/or other materials provided with the distribution.     * Neither the name of UChicago Argonne LLC nor the       names of its contributors may be used to endorse or promote products       derived from this software without specific prior written permission.</p> <p>THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL  BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."},{"location":"community/publications/","title":"Publications","text":"<p>Balsam is a community project developed at Argonne Leadership Computing Facility. If you find it useful for your computational science work, please include the following citation in your publications.</p> <ul> <li>M. Salim, T. D. Uram, J.T. Childers, P. Balaprakash, V. Vishwanath, M. Papka. Balsam: Automated Scheduling and Execution of Dynamic, Data-Intensive HPC Workflows. In Proceedings of the 8th Workshop on Python for High-Performance and Scientific Computing. ACM Press, 2018.</li> </ul>"},{"location":"community/publications/#related-publications","title":"Related Publications","text":"<ul> <li> <p>R. Vescovi, H. Li, J. Kinnison, M. Keceli, M. Salim, N. Kasthuri, T. D. Uram, N. Ferrier, Toward an Automated HPC Pipeline for Processing Large Scale Electron Microscopy Data, 2020 IEEE/ACM 2nd Annual Workshop on Extreme-scale Experiment-in-the-Loop Computing (XLOOP), 2020, pp. 16-22, doi: 10.1109/XLOOP51963.2020.00008.</p> </li> <li> <p>M. Salim, T. D. Uram, J. T. Childers, V. Vishwanath and M. Papka, Balsam: Near Real-Time Experimental Data Analysis on Supercomputers, 2019 IEEE/ACM 1st Annual Workshop on Large-scale Experiment-in-the-Loop Computing (XLOOP), 2019, pp. 26-31, doi: 10.1109/XLOOP49562.2019.00010.</p> </li> <li> <p>A. Brace, M. Salim, V. Subbiah, H. Ma, M. Emani, A. Trifa, A. R. Clyde, C. Adams, T. D. Uram, H. Yoo, A. Hock, J. Liu, V. Vishwanath, and A. Ramanathan. Stream-AI-MD: streaming AI-driven adaptive molecular simulations for heterogeneous computing platforms. Proceedings of the Platform for Advanced Scientific Computing Conference. Association for Computing Machinery, New York, NY, USA, Article 6, 1\u201313. DOI:https://doi.org/10.1145/3468267.3470578</p> </li> <li> <p>A. Al-Saadi, D. H. Ahn, Y. Babuji, K. Chard, J. Corbett, M. Hategan, S. Herbein, S. Jha, D. Laney, A. Merzky, T. Munson, M. Salim, M. Titov, M. Turilli, T. D. Uram, J. M. Wozniak, ExaWorks: Workflows for Exascale, 2021 IEEE Workshop on Workflows in Support of Large-Scale Science (WORKS), 2021, pp. 50-57, doi: 10.1109/WORKS54523.2021.00012.</p> </li> <li> <p>S. Hudson, J. Larson, J. -L. Navarro and S. M. Wild, libEnsemble: A Library to Coordinate the Concurrent Evaluation of Dynamic Ensembles of Calculations, in IEEE Transactions on Parallel and Distributed Systems, vol. 33, no. 4, pp. 977-988, 1 April 2022, doi: 10.1109/TPDS.2021.3082815.</p> </li> <li> <p>P. Balaprakash, R. Egele, M. Salim, S. Wild, V. Vishwanath, F. Xia, T. Brettin, and R. Stevens. Scalable reinforcement-learning-based neural architecture search for cancer deep learning research. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '19). Association for Computing Machinery, New York, NY, USA, Article 37, 1\u201333. DOI:https://doi.org/10.1145/3295500.3356202</p> </li> <li> <p>P. Balaprakash, M. Salim, T. D. Uram, V. Vishwanath and S. M. Wild, DeepHyper: Asynchronous Hyperparameter Search for Deep Neural Networks, 2018 IEEE 25th International Conference on High Performance Computing (HiPC), 2018, pp. 42-51, doi: 10.1109/HiPC.2018.00014.</p> </li> <li> <p>M. Kostuk, T. D. Uram, T. Evans, D. M. Orlov, M. E. Papka, and D. Schissel, Automatic Between-Pulse Analysis of DIII-D Experimental Data Performed Remotely on a Supercomputer at Argonne Leadership Computing Facility. United States: N. p., 2018. Web. https://doi.org/10.1080/15361055.2017.1390388.</p> </li> <li> <p>J. T. Childers, T. D. Uram, D. Benjamin, T. J. LeCompte, and M. E. Papka. An Edge Service for Managing HPC Workflows. In Proceedings of the Fourth International Workshop on HPC User Support Tools (HUST'17). Association for Computing Machinery, New York, NY, USA, Article 1, 1\u20138. DOI:https://doi.org/10.1145/3152493.3152557</p> </li> <li> <p>T. D. Uram, J. T. Childers, T. J. LeCompte, M. E. Papka, D. Benjamin, Achieving production-level use of HEP software at the Argonne Leadership Computing Facility. Journal of Physics: Conference Series. 664. 062063. 10.1088/1742-6596/664/6/062063. </p> </li> </ul>"},{"location":"development/client/","title":"About the API Client","text":"<p>In transitioning Balsam from a database-driven application to a multi-user, Web client-driven application, we have had to rethink how the Python API should look. Both internal Balsam components and user-written scripts need a way to  manipulate and synchronize with the central state.</p> <p>In Balsam <code>0.x</code>, users leverage direct access to the Django ORM and manipulate the database with simple APIs like: <code>BalsamJob.objects.filter(state=\"FAILED\").delete()</code>. Obviously, direct database access is not acceptable in a multi-user application. However, in cutting off access to the Django ORM, users would lose the familiar API (arguably one of Balsams' most important features) and have to drop down to writing and decoding JSON data for each request.</p> <p>The client architecture described below provides a solution to this problem with a Django ORM-inspired API.  A familiar Python object model of the data, complete with models (e.g. <code>Job</code>), managers (<code>Job.objects</code>), and Querysets (<code>Job.objects.filter(state=\"FAILED\").delete()</code>) is available. Instead of accessing a database, execution of these \"queries\" results in a REST API call.</p> <p>Internally, a <code>RESTClient</code> interface encapsulates the HTTP request and authentication logic and contains <code>Resource</code> components that map ordinary Python methods to API methods.</p> <p></p>"},{"location":"development/contributing/","title":"Contributing to Balsam","text":""},{"location":"development/contributing/#installing-balsam-for-development","title":"Installing Balsam for Development","text":"<p>For Balsam development and full-stack testing, there are some additional requirements which are not installed with standard <code>pip install -e .</code> Use <code>make install-dev</code> to install Balsam with the necessary dependencies.  Direct server dependencies (e.g. FastAPI) are pinned to help with reproducible deployments.</p> <pre><code>git clone https://github.com/argonne-lcf/balsam.git\ncd balsam\n\n# Set up Python3.7+ environment\npython3.8 -m venv env\nsource env/bin/activate\n\n# Install with pinned deployment and dev dependencies:\nmake install-dev\n\n# Set up pre-commit linting hooks:\npre-commit install\n</code></pre>"},{"location":"development/contributing/#writing-code","title":"Writing code","text":"<p>Balsam relies on several tools for consistent code formatting, linting, and type checking.  These are automatically run on each <code>git commit</code> when the <code>pre-commit install</code> has been used to install the pre-commit hooks. </p> <p>Otherwise, you should manually perform these actions before pushing new code or making pull requests.  Otherwise, the CI is likely to fail:</p> <pre><code>$ make format\n$ make lint\n$ make mypy\n</code></pre> <p>If you have made any changes to the Balsam API schemas (in <code>balsam/schemas/</code>) or the REST query parameters (in <code>balsam/server/routers/filters.py</code>), you must re-generate the client Python library:</p> <pre><code>$ make generate-api\n</code></pre> <p>This runs the <code>balsam/schemas/api_generator.py</code> to re-generate the <code>balsam/_api/models.py</code> file.  You should not edit this file by hand.</p>"},{"location":"development/contributing/#testing-locally","title":"Testing Locally","text":"<p>To run tests locally, be sure that <code>BALSAM_TEST_DB_URL</code> points to an appropriate PostgreSQL testing database (the default value of <code>postgresql://postgres@localhost:5432/balsam-test</code> assumes running a DB named <code>balsam-test</code> on localhost port 5432 as the <code>postgres</code> database user without a password.)</p> <p>The testing commands rely on <code>pytest</code> and can be gleaned from the Makefile.  To run all of the tests, simply use:</p> <pre><code>$ make all\n</code></pre> <p>If you are developing with the Docker container and have a running service, you can simply execute test commands inside the running Balsam web container (named <code>gunicorn</code> by default):</p> <pre><code>$ docker exec -e BALSAM_LOG_DIR=\"/balsam/log\" \\\n   -e BALSAM_TEST_API_URL=\"http://localhost:8000\" \\ \n   gunicorn make testcov\n</code></pre>"},{"location":"development/contributing/#ci-workflows","title":"CI Workflows","text":"<p>Currently, the following processes run in Github Actions whenever code is pushed to <code>main</code> or on new pull requests: 1. Build <code>mkdocs</code> documentation 2. Run linting, formatting, and <code>mypy</code> type checks 3. Run tests against Python3.7, 3.8, and 3.9</p> <p>Additionally, when new code is pushed or merged to <code>main</code>, the official Docker container is rebuilt and published on Docker Hub.  These Github Actions are defined and maintained within the <code>.github/</code> directory.</p>"},{"location":"development/contributing/#viewing-and-writing-documentation","title":"Viewing and Writing Documentation","text":"<p>To view the docs locally while you edit them, navigate to top-level balsam directory (where <code>mkdocs.yml</code> is located) and run:</p> <pre><code>$ mkdocs serve\n</code></pre> <p>Follow the link to the documentation. Docs are markdown files in the <code>balsam/docs</code> subdirectory and can be edited  on-the-fly.  The changes will auto-refresh in the browser window.</p> <p>You can follow mermaid.js examples to create graphs, flowcharts, sequence diagrams, class diagrams, state diagrams, etc... within the Markdown files.  For example:</p> graph TD     A[Hard] --&gt;|Text| B(Round)     B --&gt; C{Decision}     C --&gt;|One| D[Result 1]     C --&gt;|Two| E[Result 2]"},{"location":"development/contributing/#release-checklist","title":"Release checklist","text":"<ol> <li>Start with code in a state that is passing all CI Tests (green checkmark in Github Actions)</li> <li>Double check <code>make all</code>: all tests passing locally</li> <li>Update the <code>__version__</code> attribute in <code>balsam/__init__.py</code></li> <li>Make a new release commit on the <code>main</code> branch with the updated version.</li> <li>Tag the commit: <code>git tag $VERSION</code></li> <li>Push the release commit: <code>git push</code></li> <li>Push the tags: <code>git push --tags</code></li> <li>Update the PyPA <code>build</code> and <code>pip</code> tools: <code>pip install --upgrade build pip twine</code></li> <li>If it exists, clean up any old distributions: <code>rm -r dist/</code></li> <li>Build the latest Balsam distribution: <code>python -m build</code></li> <li>Check the builds: <code>python -m twine check dist/*</code> and ensure both the <code>.whl</code> and <code>.tar.gz</code> have <code>PASSED</code></li> <li>Publish to PyPI: <code>python -m twine upload dist/*</code> (This will require having the PyPI credentials stored locally)</li> </ol>"},{"location":"development/data-model/","title":"Understanding Balsam","text":"<p>Balsam is made up of:</p> <ul> <li>A centrally-managed, multi-tenant web application for securely curating HPC applications,  authoring workflows, and managing high-throughput job campaigns across one or many computing facilities.</li> <li>Distributed, user-run Balsam Sites that sync with the central API to orchestrate and carry out the workflows defined by users on a given HPC platform.</li> </ul> <p>In order to understand how Balsam is organized, one should first consider the server side entities. This graph shows the database schema of the Balsam application. Each node is a table in the database, represented by one of the model classes in the ORM. Each arrow represents a ForeignKey (or many-to-one) relationship between two tables.</p> <p></p>"},{"location":"development/data-model/#the-database-schema","title":"The Database Schema","text":"<ol> <li> <p>A <code>User</code> represents a Balsam user account.  All items in the database      are linked to a single owner (tenant), which is reflected in the connectivity of      the graph. For example, to get all the jobs belonging to <code>current_user</code>,      join the tables via <code>Job.objects.filter(app__site__user=current_user)</code></p> </li> <li> <p>A <code>Site</code> must have a globally unique <code>name</code> which corresponds to a   directory on some machine. One user can own several Balsam sites located   across one or several machines.  Each site is an independent endpoint where   applications are registered, data is transferred in and out, and Job working   directories are located. Each Balsam site runs a daemon on behalf of the   user that communicates with the central API. If a user has multiple active   Balsam Sites, then a separate daemon runs at each of them.  The authenticated   daemons communicate with the central Balsam API to fetch jobs, orchestrate the   workflow locally, and update the database state. </p> </li> <li> <p>An <code>App</code> represents a runnable application at a particular Balsam Site.      Every Balsam Site contains an <code>apps/</code> directory with Python modules      containing <code>ApplicationDefinition</code> classes.  The set of      <code>ApplicationDefinitions</code> determines the applications which may run at the      Site.  An <code>App</code> instance in the data model is merely a reference to an      <code>ApplicationDefinition</code> class, uniquely identified by the Site ID and      class path.</p> </li> <li> <p>A <code>Job</code> represents a single run of an <code>App</code> at a particular <code>Site</code>.  The      <code>Job</code> contains both application-specific data (like command line      arguments) and resource requirements (like number of MPI ranks per node)      for the run. It is important to note that Job--&gt;App--&gt;Site are      non-nullable relations, so a <code>Job</code> is always bound      to run at a particular <code>Site</code> from the moment its created.  Therefore, the corresponding Balsam service      daemon may begin staging-in data as soon as a <code>Job</code> becomes visible, as appropriate.</p> </li> <li> <p>A <code>BatchJob</code> represents a job launch script and resource request submitted      by the <code>Site</code> to the local workload manager (e.g. Slurm). Notice that      the relation of <code>BatchJob</code> to <code>Site</code> is many-to-one, and that <code>Job</code> to      <code>BatchJob</code> is many-to-one. That is, many <code>Jobs</code> run in a single      <code>BatchJob</code>, and many <code>BatchJobs</code> are submitted at a <code>Site</code> over time.</p> </li> <li> <p>The <code>Session</code> is an internal model representing an active Balsam launcher      session.  <code>Jobs</code> have a nullable relationship to <code>Session</code>; when it is not      null, the job is said to be locked by a launcher, and no other launcher      should try running it.  The Balsam session API is used by launchers       acquiring jobs concurrently to avoid race conditions.  Sessions contain a      heartbeat timestamp that must be periodically ticked to maintain the session.</p> </li> <li> <p>A <code>TransferItem</code> is created for each stage-in or stage-out task       associated with a <code>Job</code>. This permits the transfer module of the Balsam       service to group transfers according to the remote source or destination,       and therefore batch small transfers efficiently. When all the stage-in <code>TransferItems</code>       linked to a <code>Job</code> are finished, it is considered \"staged-in\" and moves ahead to       preprocessing.</p> </li> <li> <p>A <code>LogEvent</code> contains a <code>timestamp</code>, <code>from_state</code>, <code>to_state</code>, and <code>message</code> for each    state transition linked to a <code>Job</code>.  The benefit of breaking a Job's state history out into    a separate Table is that it becomes easy to query for aggregate throughput, etc... without    having to first parse and accumulate timestamps nested inside a <code>Job</code> field.</p> </li> </ol>"},{"location":"development/data-model/#the-rest-api","title":"The REST API","text":"<p>Refer to the interactive document located under the <code>/docs</code> URL of your Balsam server for detailed information about each endpoint. For instance, launch a local server with <code>docker-compose up</code> and visit localhost:8000/docs.</p> <p></p>"},{"location":"development/data-model/#user-a-note-on-auth","title":"User &amp; a note on Auth","text":"<p>Generally, Balsam will need two types of Auth to function:</p> <ol> <li>Login auth: This will likely be an pair of views providing an    OAuth flow, where Balsam redirects the user to an external auth system,    and upon successful authentication, user information is redirected back    to a Balsam callback view. For testing purposes, basic password-based login    could be used instead.</li> <li>Token auth: After the initial login, Balsam clients need a way to    authenticate subsequent requests to the API.  This can be performed with    Token authentication and a secure setup like Django REST    Knox.  Upon successful    login authentication (step 1), a Token is generated and stored (encrypted)    for the User.  This token is returned to the client in the login response.    The client then stores this token, which has some expiration date, and    includes it as a HTTP header on every subsequent request to the API (e.g.    <code>Authorization: Token 4789ac8372...</code>). This is both how Javascript web clients and automated Balsam Site services can communicate with the API.</li> </ol>"},{"location":"development/data-model/#summary-of-endpoints","title":"Summary of Endpoints","text":"HTTP Method URL Description Example usage GET /sites/ Retrieve the current user's list of sites A user checks their Balsam site statuses on dashboard POST /sites/ Create a new Site <code>balsam init</code> creates a Site and stores new <code>id</code> locally PUT /sites/{id} Update Site information Service daemon syncs <code>backfill_windows</code> periodically DELETE /sites/{id} Delete Site User deletes their Site with <code>balsam rm site</code> ----------- --------------- -------------------- ------------------- GET /apps/ Retrieve the current user's list of Apps <code>balsam ls apps</code> shows Apps across sites POST /apps/ Create a new <code>App</code> <code>balsam app sync</code> creates new <code>Apps</code> from local <code>ApplicationDefinitions</code> PUT /apps/{id} Update <code>App</code> information <code>balsam app sync</code> updates existing <code>Apps</code> with changes from local <code>ApplicationDefinitions</code> DELETE /apps/{id} Delete <code>App</code> User deletes an <code>App</code>; all related <code>Jobs</code> are deleted ----------- --------------- -------------------- ------------------- GET /jobs/ Get paginated Job lists, filtered by site, state, tags, BatchJob, or App <code>balsam ls</code> POST /jobs/ Bulk-create <code>Jobs</code> Create 1k jobs with single API call PUT /jobs/{id} Update <code>Job</code> information Tweak a single job in web UI DELETE /jobs/{id} Delete <code>Job</code> Delete a single job in web UI PUT /jobs/ Bulk-update Jobs: apply same update to all jobs matching query Restart all jobs at Site X with tag workflow=\"foo\" PATCH /jobs/ Bulk-update Jobs: apply list of patches job-wise Balsam StatusUpdater component sends a list of status updates to API ----------- --------------- -------------------- ------------------- GET /batch-jobs/ Get BatchJobs Web client lists recent BatchJobs POST /batch-jobs/ Create BatchJob Web client or ElasticQueue submits a new BatchJob PUT /batch-jobs/{id} Alter BatchJob by ID Web client alters job runtime while queued DELETE /batch-jobs/{id} Delete BatchJob by ID User deletes job before it was ever submitted PATCH /batch-jobs/ Bulk Update batch jobs by patch list Service syncs BatchJob states ----------- --------------- -------------------- ------------------- GET /sessions Get Sessions List BatchJob Web view shows \"Last Heartbeat\" for each running POST /sessions Create new <code>Session</code> Launcher <code>JobSource</code> initialized POST /sessions/{id}/acquire Acquire Jobs for launcher <code>JobSource</code> acquires new jobs to run PUT /sessions/{id} Tick <code>Session</code> heartbeat <code>JobSource</code> ticks Session periodically DELETE /sessions/{id} Destroy <code>Session</code> and release Jobs Final <code>JobSource</code> <code>release()</code> call ----------- --------------- -------------------- ------------------- GET /transfers/ List <code>TransferItems</code> Transfer module gets list of pending Transfers PUT /transfers/{id} Update <code>TransferItem</code>  State Transfer module updates status PATCH /transfers/ Bulk update <code>TransferItems</code> via patch list Transfer module bulk-updates statuses of finished transfers ----------- --------------- -------------------- ------------------- GET /events Fetch EventLogs Web client filters by Job <code>tags</code> and last 24 hours to get a quick view at throughput/utilization for a particular job type"},{"location":"development/data-model/#site","title":"Site","text":"Field Name Description <code>id</code> Unique Site ID <code>name</code> The unique site name like <code>theta-knl</code> <code>path</code> Absolute POSIX path to the Site directory <code>last_refresh</code> Automatically updated timestamp: last update to Site information <code>creation_date</code> Timestamp when Site was created <code>owner</code> ForeignKey to <code>User</code> model <code>globus_endpoint_id</code> Optional <code>UUID</code>: setting an associated endpoint for data transfer <code>num_nodes</code> Number of compute nodes available at the Site <code>backfill_windows</code> JSONField: array of <code>[queue, num_nodes, wall_time_min]</code> tuples indicating backfill slots <code>queued_jobs</code> JSONField: array of <code>[queue, num_nodes, wall_time_min, state]</code> indicating currently queued and running jobs <code>optional_batch_job_params</code> JSONField used in BatchJob forms/validation <code>{name: default_value}</code>. Taken from site config. <code>allowed_projects</code> JSONField used in BatchJob forms/validation: <code>[ name: str ]</code> <code>allowed_queues</code> JSONField used in BatchJob forms/validation: <code>{name: {max_nodes, max_walltime, max_queued}}</code> <code>transfer_locations</code> JSONField used in Job stage-in/stage-out validation: <code>{alias: {protocol, netloc}}</code>"},{"location":"development/data-model/#app","title":"App","text":"Field Name Description <code>id</code> Unique App ID <code>site</code> Foreign Key to <code>Site</code> instance containing this App <code>name</code> Short name identifying the app. <code>description</code> Text description (useful in generating Web forms) <code>name</code> Name of <code>ApplicationDefinition</code> class <code>parameters</code> Command line template or function parameters. A dict of dicts with the structure: <code>{name: {required: bool, default: str, help: str}}</code> <code>transfers</code> A dict of stage-in/stage-out slots with the structure: <code>{name: {required: bool, direction: [\"in\"|\"out\"], target_path: str, help: str}}</code> <p>The <code>App</code> model is used to merely index the <code>ApplicationDefinition</code> classes that a user has registered at their Balsam Sites. </p> <p>The <code>parameters</code> field represents \"slots\" for each adjustable command line parameter. For example, an <code>ApplicationDefinition</code> command template of <code>\"echo hello, {{first_name}}!\"</code> would result in an <code>App</code> having the <code>parameters</code> list:  <code>[ {name: \"first_name\", required: true, default: \"\", help: \"\"} ]</code>.  None of the Balsam site components use <code>App.parameters</code> internally; the purpose of mirroring this field in  the database is simply to facilitate Job validation and create App-tailored web forms.</p> <p>Similarly, <code>transfers</code>  mirrors data on the <code>ApplicationDefinition</code> for Job input and validation purposes only.</p> <p>For security reasons, the validation of Job input parameters takes place in the site-local <code>ApplicationDefinition</code> module. Even if a malicious user altered the <code>parameters</code> field in the API, they would not be able to successfully run a Job with injected parameters.</p>"},{"location":"development/data-model/#job","title":"Job","text":"Field Name Description <code>id</code> Unique Job ID <code>workdir</code> Working directory, relative to the Site <code>data/</code> directory <code>tags</code> JSON <code>{str: str}</code> mappings for tagging and selecting jobs <code>session</code> ForeignKey to <code>Session</code> instance <code>app</code> ForeignKey to <code>App</code> instance <code>parameters</code> JSON <code>{paramName: paramValue}</code> for the <code>App</code> command template parameters <code>batch_job</code> ForeignKey to current or most recent <code>BatchJob</code> instance in which this <code>Job</code> ran <code>state</code> Current state of the <code>Job</code> <code>last_update</code> Timestamp of last modification to Job <code>data</code> Arbitrary JSON data storage <code>return_code</code> Most recent return code of job <code>parents</code> Non-symmetric ManyToMany  Parent --&gt; Child relations between Jobs <code>num_nodes</code> Number of compute nodes required (&gt; 1 implies MPI usage) <code>ranks_per_node</code> Number of ranks per node (&gt; 1 implies MPI usage) <code>threads_per_rank</code> Number of logical threads per MPI rank <code>threads_per_core</code> Number of logical threads per hardware core <code>launch_params</code> Optional pass-through parameters to MPI launcher (e.g -cc depth) <code>gpus_per_rank</code> Number of GPUs per MPI rank <code>node_packing_count</code> Maximum number of instances that can run on a single node <code>wall_time_min</code> Lower bound estimate for runtime of the Job (leaving at default 0 is allowed) <p>Let workdir uniqueness be the user's problem.  If they put 2 jobs with same workdir, assume it's intentional.  We can ensure that \"stdout\" of each job goes into a file named by Job ID, so multiple runs do not collide.</p> stateDiagram-v2     created: Created     awaiting_parents: Awaiting Parents     ready: Ready     staged_in: Staged In     preprocessed: Preprocessed     restart_ready: Restart Ready     running: Running     run_done: Run Done     postprocessed: Postprocessed     staged_out: Staged Out     finished: Job Finished     run_error: Run Error     run_timeout: Run Timeout     failed: Failed      created --&gt; ready: No parents     created --&gt; awaiting_parents: Pending dependencies     awaiting_parents --&gt; ready: Dependencies finished     ready --&gt; staged_in: Transfer external data in     staged_in --&gt; preprocessed: Run preprocess script     preprocessed --&gt; running: Launch job      running --&gt; run_done: Return code 0     running --&gt; run_error: Nonzero return     running --&gt; run_timeout: Early termination      run_timeout --&gt; restart_ready: Auto retry     run_error --&gt; restart_ready: Run error handler     run_error --&gt; failed: No error handler     restart_ready --&gt; running: Launch job      run_done --&gt; postprocessed: Run postprocess script     postprocessed --&gt; staged_out: Transfer data out     staged_out --&gt; finished: Job Finished <p>A user can only access Jobs they own. The related App, BatchJob, and parents are included by ID in the serialized representation. The <code>session</code> is excluded since it is only used internally.   Reverse relationships (one-to-many) with <code>transfers</code> and <code>events</code> are also not included in the Job representation, as they can be accessed through separate API endpoints.</p> <p>The related entities are represented in JSON as follows:</p> Field Serialized Deserialized <code>id</code> Primary Key Fetch Job from user-filtered queryset <code>app_id</code> Primary Key Fetch App from user-filtered queryset <code>batch_job_id</code> Primary Key Fetch BatchJob from user-filtered queryset <code>parent_ids</code> Primary Key list Fetch parent jobs from user-filtered queryset <code>transfers</code> N/A Create only: Dict of <code>{transfer_item_name: {location_alias: str, path: str}}</code> <code>events</code> N/A N/A <code>session</code> N/A N/A <p><code>transfers</code> are nested in the Job for <code>POST</code> only: <code>Job</code> creation is an atomic transaction grouping addition of the <code>Job</code> with its related <code>TransferItems</code>. The API fetches the related <code>App.transfers</code>  and <code>Site.transfer_locations</code> to validate each transfer item:</p> <ul> <li><code>transfer_item_name</code> must match one of the keys in <code>App.transfers</code>, which      determines the <code>direction</code> and local path</li> <li>The <code>location_alias</code> must match one of the keys in <code>Site.transfer_locations</code>,      which determines the <code>protocol</code> and <code>remote_netloc</code></li> <li>Finally, the remote path is determined by the <code>path</code> key in each <code>Job</code> transfer item</li> </ul>"},{"location":"development/data-model/#batchjob","title":"BatchJob","text":"Field Name Description <code>id</code> Unique ID. Not to be confused with Scheduler ID, which is not necessarily unique across Sites! <code>site</code> ForeignKey to <code>Site</code> where submitted <code>scheduler_id</code> ID assigned by Site's batch scheduler (null if unassigned) <code>project</code> Project/allocation to be charged for the job submission <code>queue</code> Which scheduler queue the batchjob is submitted to <code>num_nodes</code> Number of nodes requested for batchjob <code>wall_time_min</code> Wall time, in minutes, requested <code>job_mode</code> Balsam launcher job mode <code>optional_params</code> Extra pass-through parameters to Job Template <code>filter_tags</code> Restrict launcher to run jobs with matching tags. JSONField dict: <code>{tag_key: tag_val}</code> <code>state</code> Current status of BatchJob <code>status_info</code> JSON: Error or custom data received from scheduler <code>start_time</code> DateTime when BatchJob started running <code>end_time</code> DateTime when BatchJob ended <p>Every workload manager is different and there are numerous job states intentionally not considered in the <code>BatchJob</code> model, including <code>starting</code>, <code>exiting</code>, <code>user_hold</code>, <code>dep_hold</code>, etc.  It is the responsibility of the site's Scheduler interface to translate real scheduler states to one of the few coarse-grained Balsam <code>BatchJob</code> states: <code>queued</code>, <code>running</code>, or <code>finished</code>.</p> stateDiagram-v2     pending_submission  --&gt; queued     pending_submission --&gt; submit_failed     queued --&gt; running     running --&gt; finished      pending_submission --&gt; pending_deletion     queued --&gt; pending_deletion     running --&gt; pending_deletion     pending_deletion --&gt; finished"},{"location":"development/data-model/#session","title":"Session","text":"Field Name Description <code>id</code> Unique ID <code>heartbeat</code> DateTime of last session tick API call <code>batch_job</code> Non-nullable ForeignKey to <code>BatchJob</code> this Session is running under <ul> <li><code>Session</code> creation only requires providing <code>batch_job_id</code>. </li> <li><code>Session</code> tick has empty payload</li> <li><code>Session</code> acquire endpoint uses a special <code>JobAcquireSerializer</code> representation:</li> </ul> Field Description <code>states</code> <code>list</code> of states to acquire <code>max_num_acquire</code> limit number of jobs to acquire <code>filter_tags</code> filter <code>Jobs</code> for which <code>job.tags</code> contains all <code>{tag_name: tag_value}</code> pairs <code>node_resources</code> Nested <code>NodeResource</code> representation placing resource constraints on what Jobs may be acquired <code>order_by</code> order returned jobs according to a set of Job fields (may include ascending or descending <code>num_nodes</code>, <code>node_packing_count</code>, <code>wall_time_min</code>) <p>The nested <code>NodeResource</code> representation is provided as a dict with the structure: <pre><code>{\n    \"max_jobs_per_node\": 1,  # Determined by Site settings for each Launcher job mode\n    \"max_wall_time_min\": 60,\n    \"running_job_counts\": [0, 1, 0],\n    \"node_occupancies\": [0.0, 1.0, 0.0],\n    \"idle_cores\": [64, 63, 64],\n    \"idle_gpus\": [1, 0, 1],\n}\n</code></pre></p>"},{"location":"development/data-model/#transferitem","title":"TransferItem","text":"Field Name Description <code>id</code> Unique TransferItem ID <code>job</code> ForeignKey to Job <code>protocol</code> <code>globus</code> or <code>rsync</code> <code>direction</code> <code>in</code> or <code>out</code>. If <code>in</code>, the transfer is from <code>remote_netloc:source_path</code> to <code>Job.workdir/destination_path</code>. If <code>out</code>, the transfer is from <code>Job.workdir/src_path</code> to <code>remote_netloc:dest_path</code>. <code>remote_netloc</code> The Globus endpoint UUID or user@hostname of the remote data location <code>source_path</code> If stage-<code>in</code>: the remote path. If stage-<code>out</code>: the local path <code>destination_path</code> If stage-<code>in</code>: the local path. If stage-<code>out</code>: the remote path. <code>state</code> <code>pending</code> -&gt; <code>active</code> -&gt; <code>done</code> or <code>error</code> <code>task_id</code> Unique identifier of the Transfer task (e.g. Globus Task UUID) <code>transfer_info</code> JSONField for Error messages, average bandwidth, transfer time, etc... There is no create (<code>POST</code>) method on the <code>/transfers</code> endpoint, because <code>TransferItem</code> creation is directly linked with <code>Job</code> creation. The related Transfers are nested in the <code>Job</code> representation when <code>POSTING</code> new jobs. The following fields are fixed at creation time: <ul> <li><code>id</code></li> <li><code>job</code></li> <li><code>protocol</code></li> <li><code>direction</code></li> <li><code>remote_netloc</code></li> <li><code>source_path</code></li> <li><code>dest_path</code></li> </ul> <p>For <code>list</code> (GET), the representation includes all fields. <code>job_id</code> represents the <code>Job</code> by primary key.</p> <p>For <code>update</code> (PUT and PATCH), only <code>state</code>, <code>task_id</code>, and <code>transfer_info</code> may be modified. The update of a state to <code>done</code> triggers a check of the related <code>Job</code>'s transfers to determine whether the job can be advanced to <code>STAGED_IN</code>.</p>"},{"location":"development/data-model/#logevent","title":"LogEvent","text":"Field Name Description <code>id</code> Unique ID <code>job</code> ForeignKey to <code>Job</code> undergoing event <code>timestamp</code> DateTime of event <code>from_state</code> Job state before transition <code>to_state</code> Job state after transition <code>data</code> JSONField containing <code>{message: str}</code> and other optional data <p>For transitions to or from <code>RUNNING</code>, the <code>data</code> includes <code>nodes</code> as a fractional number of occupied nodes. This enables clients to generate throughput and utilization views without having to fetch entire related Jobs.</p> <p>This is a read only-API with all fields included.  The related Job is represented by primary key <code>job_id</code> field.</p>"},{"location":"development/deploy/","title":"Balsam Service Deployment","text":""},{"location":"development/deploy/#installation","title":"Installation","text":""},{"location":"development/deploy/#docker-compose-quick-setup-recommended","title":"Docker Compose: quick setup (recommended)","text":"<p>Docker Compose can be used to manage the PostgreSQL,  Redis, and Balsam server containers with a single command:</p> <pre><code>$ cd balsam/\n$ git pull\n$ cp .env.example .env\n$ vim .env  # Configure here\n$ vim balsam/server/gunicorn.conf.example.py # And here\n$ docker-compose up --build -d\n</code></pre>"},{"location":"development/deploy/#manual-installation","title":"Manual Installation","text":"<p>Balsam can be installed into a Python environment in two ways.  User-mode installation with <code>pip install --pre balsam</code> or <code>pip install -e .</code> fetches end-user package dependencies with flexible version ranges.  This will not suffice for running a Balsam server. To install the server, you must use the second option:</p> <pre><code>pip install -r requirements/deploy.txt\n</code></pre> <p>This installs all the necessary dependencies with pinned versions for a reproducible deployment environment.</p> <p>Next, you will need to run a PostgreSQL database dedicated to Balsam.  If <code>which pg_ctl</code>  does not show a Postgres on your system, get the Postgres binaries. You only need to unzip and add the postgres bin/ to your PATH. Follow the PostgreSQL docuemntation to create a new database cluster with <code>initdb</code>, configure and start up the database server, and create an empty database with <code>createdb</code>. The canonical name for the database is <code>balsam</code> but any name can be used. The DSN for Balsam to connect to the database is configured via <code>BALSAM_DATABASE_URL</code>. Refer to the <code>.env.example</code> file and Pydantic documentation for URL parsing.</p> <p>Running Redis is optional and needed only for the Balsam event streaming WebSocket functionality.  Inside your virtualenv, run the <code>redis-install.sh</code> script to install Redis (or DIY). This will copy the built Redis binary in your virtualenv <code>bin/</code>. Run the Redis server and configure the Balsam-Redis connection via the <code>BALSAM_REDIS_PARAMS</code> environment variable, which should be a JSON-formatted string as shown in the <code>.env.example</code> file.</p>"},{"location":"development/deploy/#balsam-server-configuration","title":"Balsam Server Configuration","text":"<p>Regardless of installation method, the server can be configured in a few places:</p> <ol> <li>Environment variables are read from the <code>.env</code> file in the project root directory. Refer to, copy, and modify the <code>.env.example</code> example file.</li> <li>Gunicorn-specific configuration is contained in the file referenced by <code>GUNICORN_CONFIG_FILE</code>.  Refer to the example in <code>balsam/server/gunicorn.conf.example.py</code></li> <li>Any additional settings listed in <code>balsam/server/conf.py</code> can also be controlled through the <code>.env</code> file or environment variables. The environment variables should be formed by combining the appropriate <code>env_prefix</code> with the setting name.</li> </ol> <p>Keeping Balsam and Gunicorn Config Separate</p> <p>Settings internal to the Balsam web server application are defined with Pydantic in <code>balsam/server/conf.py</code>.  This includes concerns such as where to find the database, how to perform logging, and how to perform user Authentication.</p> <p>This should not be conflated with outer-level server environment concerns that Balsam itself does not need to know.  Examples include which port the server is listening on, or how many copies of the underlying <code>uvicorn</code> web worker are running.  These ultimately depend on the deployment method. In the case of Docker Compose with Gunicorn, we break the config into the separate <code>gunicorn.conf.example.py</code> file and load it from within the Dockerfile's entrypoint.</p>"},{"location":"development/deploy/#database-migrations","title":"Database Migrations","text":"<p>Initially, the PostgreSQL database will be empty and have no tables defined.  To apply the latest Balsam database schema, you need to run the Alembic migrations:</p> <pre><code># Make sure Postgres is up and running\n# Make sure .env has the correct BALSAM_DATABASE_URL\n$ balsam server migrate\n\n# Or when using Docker:\n$ docker-compose exec gunicorn balsam server migrate\n</code></pre>"},{"location":"development/deploy/#stopping-and-starting-the-server","title":"Stopping and Starting the Server","text":"<p>With Docker Compose, the server and its companion services are started/stopped with the  <code>docker-compose</code> subcommands <code>up</code> and <code>down</code>:</p> <pre><code># Start, detached:\n$ docker-compose up -d\n\n# Stop:\n$ docker-compose down\n</code></pre> <p>When running a bare-metal installation, you are responsible for having PostgreSQL (and optionally Redis) started up separately. Then, you may launch the Balsam web application with <code>gunicorn</code>:</p> <pre><code>$ gunicorn -c ./balsam/server/gunicorn.conf.example.py balsam.server.main:app\n</code></pre> <p>As a quick sanity check that the server is running and reachable, you can try to fetch the FastAPI docs:</p> <pre><code>$ curl localhost:8000/docs\n</code></pre>"},{"location":"development/deploy/#updating-the-server-code","title":"Updating the Server Code","text":"<p>Run <code>git pull</code> to update the server Python code. Because the source directory is mounted in the container, this can even be used to live-update the server when running with Docker Compose:</p> <pre><code>$ git pull\n$ docker kill --signal=SIGHUP gunicorn\n</code></pre> <p>The same applies when running without Docker.</p> <p>Live Update Limitations</p> <p>Restarting gunicorn workers with <code>SIGHUP</code> avoids server downtime, but it will not apply changes to the container environment (i.e. any changes made to <code>docker-compose.yml</code> or <code>.env</code> will not propagate to the workers).  This method is only useful for updates to the gunicorn config or Python source code.</p> <p>More generally, when there are changes to the container, Python environment, or configuration, you will want to rebuild the container and restart the service, which entails downtime:</p> <pre><code>$ cd balsam/\n$ docker-compose down\n$ docker-compose build gunicorn\n$ docker-compose up -d\n</code></pre>"},{"location":"development/deploy/#database-backups","title":"Database Backups","text":"<p>The script <code>balsam/deploy/db_snapshot.py</code> can be used with the accompanying <code>service.example</code> file to set up a recurring service for dumping the Postgres database to a file.  Copy the Python script to an appropriate location for the service, modify the <code>service.example</code> file accordingly, and follow the instructions at the top of the <code>service.example</code> file. </p> <p>The script uses the basic postgres dump functionality.  Backups are performed locally, and should be replicated to a remote system.  The easiest way within the CELS GCE environment is to set up a cron job to pull the database backups.</p> <pre><code>15 * * * *  rsync -avz balsam-dev-01.alcf.anl.gov:/home/msalim/db-backups /nfs/gce/projects/balsam/backups/\n</code></pre>"},{"location":"development/layout/","title":"Project Layout","text":""},{"location":"development/layout/#overview","title":"Overview","text":"<p>This page summarizes Balsam architecture at a high level in terms of the roles that various modules play.  Here are the files and folders you'll find right under <code>balsam/</code>:</p> <ul> <li><code>balsam.schemas</code>: The source of truth on the REST API schema is here.  This defines the various resources which are imported by FastAPI code in <code>balsam.server</code>.  FastAPI uses this to generate the OpenAPI schema and interactive documentation at <code>/docs</code>. Moreover, the user-facing SDK in <code>balsam/_api/models.py</code> is dynamically-generated from the schemas herein. Running <code>make generate-api</code> (which is invoked during <code>make all</code>) will re-generate <code>_api/models.py</code> from the schemas.</li> <li><code>balsam.server</code> the backend REST API server code that is deployed using Gunicorn, alongside PostgreSQL, to host the Balsam web service. Virtually all User or Site-initiated actions via <code>balsam.api</code> will ultimately create an HTTPS request in <code>balsam.client</code> that gets handled by this server.</li> <li><code>balsam.client</code>:  Implementation of the lower-level HTTP clients that are used by the Managers of <code>balsam._api</code>.  Auth and retry logic is here.  This is what ultimately talks to the server.</li> <li><code>balsam._api</code>:  implementation of a Django ORM-like library for interacting with the Balsam REST API (e.g. look here to understand how <code>Job.objects.filter()</code> is implemented)</li> <li><code>balsam.api</code>: public re-export of the user-facing resources in <code>_api</code></li> <li><code>balsam.analytics</code>: user-facing helper functions to analyze Job statistics</li> <li><code>balsam.cmdline</code>: The command line interfaces.</li> <li><code>balsam.config.config</code>: defines the central <code>ClientSettings</code> class, which loads credentials from <code>~/.balsam/client.yml</code>, as well as the <code>Settings</code> class, which loads Site-local settings from <code>settings.yml</code>.  To understand the magic in these classes look at Pydantic Settings management.</li> <li><code>balsam.config.site_builder</code>: Does the heavy lifting for <code>balsam site init</code>.</li> <li><code>balsam.config.defaults</code>:  The default settings for various HPC platforms that Balsam supports out-of-the-box.  Add new defaults here to extend the list of systems that users are prompted to select from <code>balsam site init.</code></li> <li><code>balsam.platform</code>: This package defines generic interfaces to compute nodes, MPI app launchers, and schedulers. Concrete subclasses of these implement the platform-specific interfaces (e.g. Theta <code>aprun</code> launcher or Slurm scheduler)</li> <li><code>balsam.shared_apps</code>: pre-packaged <code>ApplicationDefinitions</code> that users can import, subclass, and deploy to their Sites. </li> <li><code>balsam.site</code>: The implementation of the Balsam Site Agent and the Launcher pilot jobs that it ultimately dispatches. Thanks to the generic platform interfaces, all the code that does heavy lifting here is totally platform-agnostic: if you find yourself modifying code in here to accomodate new systems, you're doing it wrong!</li> <li><code>balsam.util</code>: logging, signal handling, datetime parsing, and miscellaneous helpers.</li> </ul>"},{"location":"development/layout/#balsamschemas","title":"<code>balsam.schemas</code>","text":"<p>Pydantic is used to define the REST API data structures, (de-)serialize HTTP JSON payloads, and perform validation. The schemas under <code>balsam.schemas</code> are used both by the user-facing <code>balsam._api</code> classes and the backend <code>balsam.server.routers</code> API. Thus when an update to the schema is made, both the client and server-side code inherit the change.</p> <p>The script <code>schemas/api_generator.py</code> is invoked to re-generate the <code>balsam/_api/models.py</code> whenever the schema changes. Thus, users benefit from always-up-to-date docstrings and type annotations across the Balsam SDK, while the implementation is handled by the internal Models, Managers, and Query classes in <code>balsam/_api</code>.</p>"},{"location":"development/layout/#balsamserver","title":"<code>balsam.server</code>","text":"<p>This is the self-contained codebase for the API server, implemented with FastAPI and SQLAlchemy.  We do not expect Balsam users to ever run or touch this code, unless they are interested in standing up their own server.</p> <ul> <li><code>server/conf.py</code> contains the server settings class which loads server settings from the environment using Pydantic.</li> <li><code>server/main.py</code> imports all of the API routers which define the HTTP endpoints for <code>/auth/</code>, <code>/jobs</code>, etc...</li> <li><code>server/auth</code> contains authentication routes and logic.<ul> <li><code>__init__.py::build_auth_router</code> uses the server settings to determine which login methods will be exposed under the API <code>/auth</code> URLs.  </li> <li><code>authorization_code_login.py</code> and <code>device_code_login.py</code> comprise the OAuth2 login capability.</li> <li><code>db_sessions.py</code> defines the <code>get_admin_session</code> and <code>get_webuser_session</code> functions that manage database connections and connection pooling.  The latter can be used to obtain user-level sessions with RLS (row-level security).</li> <li><code>token.py</code> has the JWT logic which is used on every single request (not just login!) to authenticate the client request prior to invoking the FastAPI route handler.</li> </ul> </li> <li><code>server.main</code> defines the top-level URL routes into views located in <code>balsam.server.routers</code></li> <li><code>balsam.server.routers</code> defines the possible API actions.  These routes ultimately call into various methods under <code>balsam.server.models.crud</code>, where the business logic is defined.</li> <li><code>balsam.server.models</code> encapsulates the database and any actions that involve database communication.<ul> <li><code>alembic</code> contains the database migrations</li> <li><code>crud</code> contains the business logic invoked by the various FastAPI routes</li> <li><code>tables.py</code> contains the SQLAlchemy model definitions</li> </ul> </li> </ul>"},{"location":"development/layout/#balsamclient","title":"<code>balsam.client</code>","text":"<p>This package defines the low-level <code>RESTClient</code> interface used by all the Balsam Python clients. The implementations capture the details of authentication to the Balsam API and performing HTTP requests.</p>"},{"location":"development/layout/#balsam_api","title":"<code>balsam._api</code>","text":"<p>Whereas the <code>RESTClient</code> provides a lower-level interface (exchanging JSON data over HTTP), the <code>balsam._api</code> defines Django ORM-like <code>Models</code> and <code>Managers</code> to emulate the original Balsam API:</p> <pre><code>from balsam.api import Job\n\nfinished_jobs = Job.objects.filter(state=\"JOB_FINISHED\")\n</code></pre> <p>This is the primary user-facing SDK.  Under the hood, it uses a <code>RESTClient</code> implementation  from the <code>balsam.client</code> subpackage to actually make HTTP requests.</p> <p>For example, the <code>Job</code> model has access via <code>Job.objects</code> to an instance of <code>JobManager</code> which in turn was initialized with an authenticated <code>RESTClient</code>.  The Manager is responsible for auto-chunking large requests, lazy-loading queries, handling pagination transparently, and other features inspired by Django ORM that go beyond a typical auto-generated OpenAPI SDK.  Rather than emitting SQL, these Models and Managers work together to generate HTTP requests.</p>"},{"location":"development/layout/#applicationdefinition","title":"<code>ApplicationDefinition</code>","text":"<p>Users write their own subclasses of <code>ApplicationDefinition</code> (defined in <code>_api/app.py</code>) to configure the Apps that may run at a particular Balsam Site.</p> <p>Each <code>ApplicationDefinition</code> is serialized and synchronized with the API when users run the <code>sync()</code> method or submit Jobs using the App.</p>"},{"location":"development/layout/#balsamplatform","title":"<code>balsam.platform</code>","text":"<p>The <code>platform</code> subpackage contains all the platform-specific interfaces to various HPC systems. The goal of this architecture is to make porting Balsam to new HPC systems easier: a developer should only have to write minimal interface code under <code>balsam.platform</code> that subclasses and implements well-defined interfaces.</p> <p>Here is a summary of the key Balsam platform interfaces:</p>"},{"location":"development/layout/#apprun","title":"<code>AppRun</code>","text":"<p>This is the application launcher (<code>mpirun</code>, <code>aprun</code>, <code>jsrun</code>) interface used by the Balsam launcher (pilot job).  It encapsulates the environment, workdir, output streams, compute resource specification (such as MPI ranks and GPUs), and the running process. <code>AppRun</code> implementations may or may not use a subprocess implementation to invoke the run.</p>"},{"location":"development/layout/#computenode","title":"<code>ComputeNode</code>","text":"<p>The Balsam launcher uses this interface to discover available compute resources within a batch job, as well as to enumerate resources (CPU cores, GPUs) on a node and track their occupancy.</p>"},{"location":"development/layout/#scheduler","title":"<code>Scheduler</code>","text":"<p>The Balsam Site uses this interface to interact with the local resource manager (e.g. Slurm, Cobalt) to submit new batch jobs, check on job statuses, and inspect other system-wide metrics (e.g. backfill availability).</p>"},{"location":"development/layout/#transferinterface","title":"<code>TransferInterface</code>","text":"<p>The Balsam Site uses this interface to submit new transfer tasks and poll on their status. A GlobusTransfer interface is implemented for batching Job stage-ins/stage-outs into Globus Transfer tasks.</p>"},{"location":"development/layout/#balsamconfig","title":"<code>balsam.config</code>","text":"<p>A comprehensive configuration is stored in each Balsam Site as <code>settings.yml</code>. This per-site config improves isolation between sites, and enables more flexible configs when multiple sites share a filesystem.</p> <p>The Settings are also described by a Pydantic schema, which is used to validate the YAML file every time it is loaded.  The loaded settings are held in a <code>SiteConfig</code> instance that's defined within this subpackage. </p> <p>The SiteConfig is available to all users via: <code>from balsam.api import site_config</code>.  This import statement depends on the resolution of the Balsam site path from the local filesystem (i.e. it can only work when <code>cwd</code> is inside of a Balsam site, which is the case wherever a <code>Job</code> is running or pre/post-proccesing.)</p>"},{"location":"development/layout/#balsamsite","title":"<code>balsam.site</code>","text":"<p>This subpackage contains the real functional core of Balsam: the various components that run on a Site to execute workflows.  The Site <code>settings.yml</code> specifies which platform adapters are needed, and these adapters are injected into the Site components, which use them generically.  This enables all the code under <code>balsam.site</code> to run  across platforms without modification.</p>"},{"location":"development/layout/#jobsource","title":"<code>JobSource</code>","text":"<p>Launchers and pre/post-processing modules use this interface to fetch <code>Jobs</code> from the API. The abstraction keeps specific API calls out of the launcher code base, and permits different implementation strategies:</p> <ul> <li><code>FixedDepthJobSource</code> maintains a queue of pre-fetched jobs using a background process</li> <li><code>SynchronousJobSource</code> performs a blocking API call to fetch jobs according to a specification of available resources.</li> </ul> <p>Most importantly, both <code>JobSources</code> use the Balsam <code>Session</code> API to acquire Jobs by performing an HTTP <code>POST /sessions/{session_id}</code>.  This prevents race conditions when concurrent launchers acquire Jobs at the same Site, and it ensures that Jobs are not locked in perpetuity if a Session expires. Sessions are ticked with a periodic heartbeat to refresh the lock on long-running jobs. Eventually, Sessions are deleted when the corresponding launcher ends.  Details of the job acquisition API are in <code>schemas/sessions.py::SessionAcquire</code>.</p>"},{"location":"development/layout/#statusupdater","title":"<code>StatusUpdater</code>","text":"<p>The <code>StatusUpdater</code> interface is used to manage job status updates, and also helps to keep API-specific code out of the other Balsam internals. The primary implementation <code>BulkStatusUpdater</code> pools update events that are passed via queue to a background process, and performs bulk API updates to reduce the frequency of API calls.</p>"},{"location":"development/layout/#scripttemplate","title":"<code>ScriptTemplate</code>","text":"<p>The <code>ScriptTemplate</code> is used to generate shell scripts for submission to the local resource manager, using a Site-specific job template file.</p>"},{"location":"development/layout/#launcher","title":"<code>Launcher</code>","text":"<p>The <code>MPI</code> and <code>serial</code> job modes of the Balsam launcher are implemented here. These are standalone, executable Python scripts that carry out the execution of Balsam <code>Jobs</code> (sometimes called a pilot job mechanism). The launchers are invoked from a shell script generated by the <code>ScriptTemplate</code> which is submitted to the local resource manager (via the <code>Scheduler</code> interface).</p> <p>The <code>mpi</code> mode is a simpler implementation that runs a single process on the head node of a batch job which launches each job with the MPI launcher (e.g. <code>aprun</code>, <code>mpiexec</code>).  The <code>serial</code> mode is more involved: a Worker process is first started on each compute node; these workers then receive <code>Jobs</code> and send status updates to a Master process.  The advantage of Serial mode is that very large numbers of node-local jobs can be launched without incurring the overhead of a single <code>mpiexec</code> per job.</p> <p>In both <code>mpi</code> and <code>serial</code> modes, the leader process uses a <code>JobSource</code> to acquire jobs and <code>StatusUpdater</code> to send state updates back to the API (see above).</p> <p>Serial mode differs from MPI mode by using ZeroMQ to forward jobs from the leader to the Workers.  Moreover, users can pass the <code>--partitions</code> flag to split a single Batch job allocation into multiple leader/worker groups.  This allows for scalable launch to hundreds of thousands of simultaneous tasks.  For instance on ThetaKNL, one can launch 131,072 simultaneous jobs across 2048 nodes by packing 64 Jobs per node with <code>node_packing_count=64</code>.  The Workers will prefetch Jobs and launch them, thereby hiding the overhead of communication with the leader.  To further speed this process, the 2048 node batch job can be split into <code>16</code> partitions of <code>128 nodes</code> each. Thus: 1 central API server fans out to 16 serial mode leaders, each of which fan out to 128 workers, and each worker prefetches Jobs and launches 64 concurrently. Job launch and polling are almost entirely overlapped with communication in this paradigm, and the serial mode leaders use background processes for the <code>JobSource</code> and <code>StatusUpdater</code>, leaving the leader highly available to forward jobs to Workers and route updates back to the <code>StatusUpdater</code>'s queue.</p>"},{"location":"development/layout/#balsamsiteservice","title":"<code>balsam.site.service</code>","text":"<p>The Balsam Site daemon comprises a group of background processes that run on behalf of the user. The daemon may run on a login node, or on any other resource appropriate for a long-running background process. The only requirements are that:</p> <ul> <li>The Site daemon can access the filesystem with the Site directory, and</li> <li>The Site daemon can access the local resource manager (e.g. perform <code>qsub</code>)</li> <li>The Site daemon can access the Balsam API</li> </ul> <p>The Site daemon is organized as a collection of <code>BalsamService</code> classes, each of which describes a particular background process. This setup is highly modular: users can easily configure which service modules are in use, and developers can implement additional services that hook directly into the Site.</p>"},{"location":"development/layout/#main","title":"<code>main</code>","text":"<p>The <code>balsam/site/service/main.py</code> is the entry point that ultimately loads <code>settings.yml</code> into a <code>SiteConfig</code> instance, which defines the various services that the Site Agent will run.  Each of these services is launched as a background process and monitored here after invoking <code>balsam site start</code>.  When terminated (e.g. with <code>balsam site stop</code>) -- the teardown happens here as well.</p> <p>The following are some of the most common plugins to the Balsam site agent.</p>"},{"location":"development/layout/#schedulerservice","title":"<code>SchedulerService</code>","text":"<p>This <code>BalsamService</code> component syncs with <code>BatchJobs</code> in the Balsam API and uses the <code>Scheduler</code> platform interface to submit new <code>BatchJobs</code> and update the status of existing <code>BatchJobs</code>. It does not automate the process of job submission -- it only serves to keep the API state and local resource manager state synchronized.</p> <p>For example, a user performs <code>balsam queue submit</code> to add a new <code>BatchJob</code> to the REST API. The <code>SchedulerService</code> eventually detects this new <code>BatchJob</code>, generates an appropriate script from the <code>ScriptTemplate</code> and <code>job-template.sh</code> (see above), and submits it to the local Slurm scheduler.</p>"},{"location":"development/layout/#elasticqueueservice","title":"<code>ElasticQueueService</code>","text":"<p>This <code>BalsamService</code> monitors the backlog of <code>Jobs</code> and locally available compute resources, and it automatically submits new <code>BatchJobs</code> to the API to adapt to realtime workloads. This is a form of automated job submission, which works together with the <code>SchedulerService</code> to fully automate resource allocation and execution.</p>"},{"location":"development/layout/#queuemaintainerservice","title":"<code>QueueMaintainerService</code>","text":"<p>This is another, simpler, form of automated job submission, in which a constant number of fixed-size <code>BatchJobs</code> are maintained at a Site (e.g. keep 5 jobs queued at all times). Intended to get through a long campaign of runs.</p>"},{"location":"development/layout/#processingservice","title":"<code>ProcessingService</code>","text":"<p>This service carries out the execution of various workflow steps that are defined on the <code>ApplicationDefinition</code>:</p> <ul> <li><code>preprocess()</code></li> <li><code>postprocess()</code></li> <li><code>handle_error()</code></li> <li><code>handle_timeout()</code></li> </ul> <p>These are meant to be lightweight and IO-bound tasks that run in a process pool on the login node or similar resource. Compute-intensive tasks should be performed in the main body of an App.</p>"},{"location":"development/layout/#transferservice","title":"<code>TransferService</code>","text":"<p>This service automates staging in data from remote locations prior to the <code>preprocess()</code> step of a Job, and staging results out to other remote locations after <code>postprocess()</code>. The service batches files and directories that are to be moved between a certain pair of endpoints, and creates batch Transfer tasks via the <code>TransferInterface</code>.</p>"},{"location":"development/layout/#balsamcmdline","title":"<code>balsam.cmdline</code>","text":"<p>The command line interfaces to Balsam are written as Python functions decorated with Click</p>"},{"location":"development/porting/","title":"Porting Balsam to new HPC Sites","text":"<p>Porting Balsam to a new system requires minimal (or no) code. We simply need to provide an off-the-shelf default configuration that users of the system can bootstrap new Sites from. </p>"},{"location":"development/porting/#select-the-platform-interfaces","title":"Select the Platform Interfaces","text":"<p>To port Balsam to a new system, one only needs to select three compatible interfaces:</p> <ol> <li><code>AppRun</code>: The MPI application launcher class</li> <li><code>ComputeNode</code>: The node resource definition class</li> <li><code>SchedulerInterface</code>: The HPC resource manager (batch scheduler) class</li> </ol> <p>Several interfaces are implemented in the respective platform directories: <code>platform/app_run</code>, <code>platform/compute_node</code>, and <code>platform/scheduler</code>.  If the interface to your system is missing, simply add a new subclass that copies the structure of an existing, related implementation.  In most cases, the necessary changes are minimal.  New interfaces should be included in the appropriate <code>__init__.py</code> for uniform accessibility.</p>"},{"location":"development/porting/#create-a-default-configuration","title":"Create a Default Configuration","text":"<p>Create a new configuration folder for your platform under <code>balsam/config/defaults/</code>. Inside, you will need to add the following:</p> <ul> <li><code>apps/__init__.py</code> (and other default apps therein)</li> <li><code>settings.yml</code> (Referencing the platform interfaces added above)</li> <li><code>job-template.sh</code></li> </ul> <p>Again, the easiest way is to copy an example from one of the existing folders in <code>balsam/config/defaults/</code>. The <code>job-template.sh</code> is used to generate the shell scripts that are ultimately submitted to the HPC scheduler. This is where any necessary scheduler flags or <code>module load</code> statements can be added.</p>"},{"location":"development/testing/","title":"Integration Testing Balsam on new HPC platforms","text":""},{"location":"development/testing/#registering-a-new-test-platform","title":"Registering a new test platform","text":"<p>In <code>tests/test_platform.py</code>:</p> <p>Add a supported platform string to the <code>PLATFORMS</code> set:</p> <pre><code>PLATFORMS: Set[str] = {\"alcf_theta\", \"alcf_thetagpu\", \"alcf_cooley\", \"generic\"}\n</code></pre> <p><code>tests/test_platform.py</code> also references some environment variables that any test runner (CI or manual) needs to set:</p> <ul> <li><code>BALSAM_TEST_DIR</code>: ephemeral test site will be created in this directory; should be somewhere readable from both the test machine and the compute/launch nodes.</li> <li><code>BALSAM_LOG_DIR</code>: artifacts of the run saved here</li> <li><code>BALSAM_TEST_API_URL</code>: http:// URL of the server to test against</li> <li><code>BALSAM_TEST_PLATFORM</code>: the PLATFORM value such as \"alcf_theta\"</li> </ul> <p><code>BALSAM_TEST_DB_URL</code> is ignored if you are testing against an existing API server with <code>BALSAM_TEST_API_URL</code>.</p> <p>Also important in the <code>test_platform.py</code> is the dictionary <code>LAUNCHER_STARTUP_TIMEOUT_SECONDS</code> describing how long the test runner should wait for a launcher to start.  This is highly platform dependent and should be set based on the expected queueing time in the test queue.</p>"},{"location":"development/testing/#test-site-configuration","title":"Test Site configuration","text":"<p>Next in <code>balsam/tests/default-configs/</code> there is a default Site config directory for each value of  <code>PLATFORMS</code>. You will need to add a config directory with a name matching the platform string added to <code>PLATFORMS</code>. </p> <p>This is basically just a clone of the user-facing default Site config located under:  <code>balsam/config/defaults/</code>.  However, the Site should be configured with any Apps or resources needed by the platform tests. For instance, the integration tests specifically require the <code>hello.Hello</code> to exist in every Site.  This App can be copied from the <code>balsam/tests/default-configs/generic/apps/hello.py</code>. You may define additional platform specific Apps here. Moreover, the tests run under the first <code>project</code> and <code>queue</code> as defined in <code>settings.yml</code>.</p>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":"<p>You should have installed the development dependencies into a virtualenv with <code>make install-dev</code>.  </p> <p>To run the integration tests from a login node: <code>cd</code> into the balsam root directory, set the environment variables above, and run <code>make test-site-integ</code>.</p>"},{"location":"development/testing/#writing-platform-specific-tests","title":"Writing platform-specific tests","text":"<p>The actual tests are detected from files named <code>tests/site_integration/test_*.py</code>. A generic test <code>test_multi_job</code> runs 3 <code>hello.Hello</code> jobs and waits for them to reach JOB_FINISHED, repeating the test with both <code>serial</code> and <code>mpi</code> job modes.</p> <p>By default, new test cases will run on every platform. </p> <p>To define a platform-specific test, simply use the <code>pytest.mark</code> decorator. This indicates that a test case should only run when the  <code>BALSAM_TEST_PLATFORM</code> environment variable matches the platform marker name:</p> <pre><code>@pytest.mark.alcf_theta\ndef test_ATLAS_workflow() -&gt; None:\n    assert 1\n</code></pre>"},{"location":"development/testing/#using-the-test-fixtures","title":"Using the test fixtures","text":"<p>The PyTest fixtures (defined in <code>conftest.py</code> files and the various test files) are responsible for all the test setup and teardown. Some useful fixtures include:</p> <ul> <li><code>balsam_site_config</code>: creates an ephemeral Site and returns the SiteConfig object</li> <li><code>client</code> returns a Balsam API client authenticated as the test Site owner</li> <li><code>live_launcher</code> is a fixture parameterized in the job mode, meaning any test using it will be automatically repeated twice with both <code>serial</code> and <code>mpi</code> job modes running on a single node.  The fixture blocks until the launcher is actually up and running (this is where the <code>LAUNCHER_STARTUP_TIMEOUT_SECONDS</code> environment variable comes into play.)</li> </ul> <p>For instance,  the following snippet applies the <code>live_launcher</code> fixture  to all test cases in the <code>TestSingleNodeLaunchers</code> class:</p> <pre><code>@pytest.mark.usefixtures(\"live_launcher\")\nclass TestSingleNodeLaunchers:\n    @pytest.mark.parametrize(\"num_jobs\", [3])\n    def test_multi_job(self, balsam_site_config: SiteConfig, num_jobs: int, client: RESTClient) -&gt; None:\n        \"\"\"\n        3 hello world jobs run to completion\n        \"\"\"\n        ...\n</code></pre>"},{"location":"tutorials/quickstart/","title":"Getting started","text":"<p>This tutorial gets you up and running with a new Balsam Site quickly.  Since Balsam is highly platform-agnostic, you can follow along by choosing from any of the available default site setups:</p> <ul> <li>A local MacOS or Linux system</li> <li>Aurora</li> <li>Perlmutter</li> <li>Polaris</li> <li>Sunspot</li> </ul>"},{"location":"tutorials/quickstart/#install","title":"Install","text":"<p>First create a new virtualenv and install Balsam:</p> <pre><code>$ python -m venv my-env\n$ source my-env/bin/activate\n$ pip install --pre balsam\n</code></pre>"},{"location":"tutorials/quickstart/#log-in","title":"Log In","text":"<p>Now that <code>balsam</code> is installed, you need to log in.  Logging in fetches an access token that is used to identify you  in all future API interactions, until the token expires and you have to log in again.</p> <pre><code>$ balsam login\n# Follow the prompt to authenticate\n</code></pre> <p>Once you are logged in, you can create a Balsam Site for job execution, or send jobs to any of your other existing Sites.</p> <p>Login temporarily restricted</p> <p>Balsam is currently in a pre-release stage and the web service is hosted on limited resources.  Consequently, logins are limited to pre-authorized users.  Please contact the ALCF Help Desk to request early access membership to the Balsam user group. </p>"},{"location":"tutorials/quickstart/#create-a-balsam-site","title":"Create a Balsam Site","text":"<p>All Balsam workflows are namespaced under Sites: self-contained project spaces belonging to individual users. You can use Balsam to manage Sites on multiple HPC systems from a single shell or Python program. This is one of the key strengths of Balsam: the usage looks exactly the same whether you're running locally or managing jobs across multiple supercomputers.</p> <p>Let's start by creating a Balsam Site in a folder named <code>./my-site</code>:</p> <pre><code>$ balsam site init ./my-site\n# Select the default configuration for your machine, e.g. Polaris\n</code></pre> <p>You will be prompted to select a default Site configuration and to enter a unique name for your Site.  The directory <code>my-site/</code> will then be created and preconfigured for the chosen platform.  You can list your Sites with the <code>balsam site ls</code> command.</p> <pre><code>$ balsam site ls\n\n   ID             Name   Path                               Active\n   21       theta-demo   .../projects/datascience/my-site   No \n</code></pre> <p>In order to actually run anything at the Site, you have to enter the Site directory and start it with <code>balsam site start</code>.  This command launches a persistent background agent which uses your access token to sync with the Balsam service.</p> <pre><code>$ cd my-site\n$ balsam site start\n</code></pre>"},{"location":"tutorials/quickstart/#set-up-your-apps","title":"Set up your Apps","text":"<p>Every Site has its own collection of Balsam Apps, which define the runnable applications at that Site.   A Balsam App is declared by writing an <code>ApplicationDefinition</code> class and running the <code>sync()</code> method from a Python program or interactive session. The simplest <code>ApplicationDefinition</code> is just a template for a <code>bash</code> command, with any workflow variables enclosed in double-curly braces:</p> <pre><code>from balsam.api import ApplicationDefinition\n\nclass Hello(ApplicationDefinition):\n    site = \"my-site\"\n    command_template = \"echo Hello, {{ say_hello_to }}!\"\n\nHello.sync()\n</code></pre> <p>Notice the attribute <code>site = \"my-site\"</code> which is required to associate the App <code>\"Hello\"</code> to the Site <code>\"my-site\"</code>. </p> <p>In addition to shell command templates, we can define Apps that invoke a Python <code>run()</code> function on a compute node:</p> <pre><code>class VecNorm(ApplicationDefinition):\n    site = \"my-site\"\n\n    def run(self, vec):\n        return sum(x**2 for x in vec)**0.5\n\nVecNorm.sync()\n</code></pre> <p>After running the <code>sync()</code> methods for these Apps, they are serialized and shipped into the Balsam cloud service.  We can then load and re-use these Apps when submitting Jobs from other Python programs.</p>"},{"location":"tutorials/quickstart/#add-jobs","title":"Add Jobs","text":"<p>With these App classes in hand, we can now submit some jobs from the Python SDK. Let's create a Job for both the <code>Hello</code> and <code>VecNorm</code> apps:  all we need is to  pass a working directory and any necessary parameters for each:</p> <pre><code>hello = Hello.submit(workdir=\"demo/hello\", say_hello_to=\"world\")\nnorm = VecNorm.submit(workdir=\"demo/norm\", vec=[3, 4])\n</code></pre> <p>Notice how shell command parameters (for <code>Hello</code>) and Python function parameters (for <code>VecNorm</code>) are treated on the same footing.  We have now created two Jobs that will eventually run on the Site <code>my-site</code>, once compute resources are available.  These Jobs can be seen by running <code>balsam job ls</code>.</p>"},{"location":"tutorials/quickstart/#make-it-run","title":"Make it run","text":"<p>A key concept in Balsam is that Jobs only specify what to run, and you must create BatchJobs to provision the resources that actually execute your jobs. This way, your workflow definitions are neatly separated from the concern of what allocation they run on.  You create a collection of Jobs first, and then many of these Jobs can run inside one (or more) BatchJobs.</p> <p>Since BatchJobs dynamically acquire Jobs, Balsam execution is fully elastic (just spin up more nodes by adding another BatchJob) and migratable (a Job that ran out of time in one batch allocation will get picked up in the next BatchJob).  BatchJobs will automatically run as many Jobs as they can at their Site.  You can simply queue up one or several BatchJobs and let them divide and conquer your workload. </p> <pre><code>BatchJob.objects.create(\n    site_id=hello_job.site_id,\n    num_nodes=1,\n    wall_time_min=10,\n    job_mode=\"mpi\",\n    queue=\"local\",  # Use the appropriate batch queue, or `local`\n    project=\"local\",  # Use the appropriate allocation, or `local`\n)\n</code></pre> <p>After running this command, the Site agent will fetch the new BatchJob and perform the necessary pilot job submission with the local resource manager (e.g. via <code>qsub</code>).  The <code>hello</code> and <code>norm</code> Jobs will run in the <code>workdirs</code> specified above, located relative to the Site's <code>data/</code> directory.  You will find the \"Hello world\" job output in a <code>job.out</code> file therein.  </p> <p>For Python <code>run()</code> applications, the created <code>Jobs</code> can be handled like <code>concurrent.futures.Future</code> instances, where the <code>result()</code> method delivers the return value (or re-raises the Exception) from a <code>run()</code> invocation.</p> <pre><code>assert norm.result() == 5.0  # (3**2 + 4**2)**0.5\n</code></pre> <p>When creating BatchJobs, you can verify that the allocation was actually created by checking with the local resource manager (e.g.  <code>qstat</code>) or by checking with Balsam:</p> <pre><code>$ balsam queue ls\n</code></pre> <p>When the BatchJob with <code>job_mode=\"mpi\"</code> starts, an MPI mode launcher pilot job acquires and runs the jobs. You will find helpful logs in the <code>logs/</code>  directory showing what's going on.  Follow the Job statuses with <code>balsam job ls</code>:</p> <pre><code>$ balsam job ls\n\nID       Site      App          Workdir   State          Tags  \n267280   my-site   Hello        test/2    JOB_FINISHED   {}    \n267279   my-site   Hello        test/1    JOB_FINISHED   {} \n</code></pre>"},{"location":"tutorials/quickstart/#a-complete-python-example","title":"A complete Python example","text":"<p>Now let's combine the Python snippets from above to show a self-contained example of Balsam SDK usage (e.g. something you might run from a Jupyter notebook):</p> <pre><code>from balsam.api import ApplicationDefinition, BatchJob, Job\n\nclass VecNorm(ApplicationDefinition):\n    site = \"my-local-site\"\n\n    def run(self, vec):\n        return sum(x**2 for x in vec)**0.5\n\njobs = [\n    VecNorm.submit(workdir=\"test/1\", vec=[3, 4]),\n    VecNorm.submit(workdir=\"test/2\", vec=[6, 8]),\n]\n\nBatchJob.objects.create(\n    site_id=jobs[0].site_id,\n    num_nodes=1,\n    wall_time_min=10,\n    job_mode=\"mpi\",\n    queue=\"local\",\n    project=\"local\",\n)\n\n# This check will block until jobs are completed; \n# remove these lines if you don't want to wait for jobs to finish\nfor job in Job.objects.as_completed(jobs):\n    print(job.workdir, job.result())\n</code></pre> <p>Note that this example script will block until the jobs are completed.</p>"},{"location":"tutorials/quickstart/#submitting-jobs-from-the-command-line","title":"Submitting Jobs from the command line","text":"<p>You can check the Apps registered at a given Site, or across all Sites, from the command line:</p> <pre><code>$ balsam app ls --site=all\n\n   ID                 Name   Site\n  286                Hello   laptop\n  287              VecNorm   laptop\n</code></pre> <p>To create a Balsam Job from the CLI, you must identify the App on the command line:</p> <pre><code>$ balsam job create --site=laptop --app Hello --workdir demo/hello2 --param say_hello_to=\"world2\" \n</code></pre> <p>There are many additional CLI options in job creation, which can be summarized with <code>balsam job create --help</code>.  You will usually create jobs using Python, but the CLI remains useful for monitoring workflow status:</p> <pre><code>$ balsam job ls\n\nID       Site     App        Workdir          State          Tags\n501649   laptop   Hello      test/1           PREPROCESSED   {}\n501650   laptop   Hello      test/2           STAGED_IN      {}\n</code></pre> <p>BatchJobs can be submitted from the CLI, with parameters that mimic a standard scheduler interface: <pre><code># Substitute -q QUEUE and -A ALLOCATION for your project:\n$ balsam queue submit -q debug -A datascience -n 1 -t 10 -j mpi \n</code></pre></p>"},{"location":"user-guide/api/","title":"The Python API","text":"<p>The documentation alludes to the Balsam Python API in several places.  For instance, the Managing Jobs section gives real-world examples of API usage in creating, querying, and updating Jobs.  In this section, we take a step back and look more generally at the structure and semantics of Balsam's Python API.  This is useful because all the Python API resources (<code>Job</code>, <code>BatchJob</code>, <code>EventLog</code>) share the same methods and data structures for creating, querying, updating, and deleting resources.</p> <p>The first thing to understand is that the Python API is merely a convenience layer built on top of a standard HTTP  requests client. One could bypass <code>balsam.api</code> altogether and interact with the Balsam REST API using another programming language or a command-line tool like <code>curl</code>.</p> <p>Resources are imported at the top-level from <code>balsam.api</code>:</p> <pre><code>from balsam.api import (\n    Site,\n    ApplicationDefinition,\n    Job,\n    BatchJob,\n    TransferItem,\n    EventLog,\n)\n</code></pre> <p>The following sections use <code>Job</code> as an example but easily generalize to any of the resources imported above. For example, querying your <code>Sites</code> looks just like querying your <code>Jobs</code>:</p> <pre><code>from datetime import datetime, timedelta\n\nhour_ago = datetime.utcnow() - timedelta(hours=1)\n\nrecently_used_sites = Site.objects.filter(last_refresh_after=hour_ago)\nrecently_updated_jobs = Job.objects.filter(last_update_after=hour_ago)\n</code></pre> <p>Once again, the docstrings and type annotations visible in a Python IDE are hugely helpful in discovering the allowed parameters for various resources. Best of all, because the Python API is dynamically generated from the REST API schema, the Python docstrings and type hints stay up to date, even if this user guide lags behind! </p>"},{"location":"user-guide/api/#model-fields","title":"Model Fields","text":"<p>Each of the API resources has a model class (like <code>Job</code>) which defines a set of Fields that can be exchanged with the REST API. The model fields are type-annotated and can be explored from your IDE or class docstrings.  For example, we can start searching for \"param\" and find that <code>job.parameters</code> should be a dictionary of strings:</p> <p></p> <p>Relationship to Django ORM</p> <p>Previous versions of Balsam used the real Django ORM to communicate with a private user database.  This proved to be an effective programming model, and so the new Balsam Python API was written to preserve a subset of the structure and syntax of the former API.</p> <p>If you peek below the surface, the current Balsam Python API is completely different from an ORM. Whereas ORMs lazily execute SQL queries over a database connection, the Balsam API executes HTTPS requests over an Internet connection. It wraps one specific REST API schema and is therefore vastly narrower in scope and capabilities.</p>"},{"location":"user-guide/api/#creating","title":"Creating","text":"<p>Each resource has three methods of construction.  First, you can create  several in-memory instances, and later persist them to the backend:</p> <pre><code># Create in-memory with the required kwargs...\nj = Job(...)\n\n# New in-memory resources have no ID:\nassert j.id is None\n\n# ...then persist:\nj.save()\nassert j.id is not None\n</code></pre> <p>Second, you can create and persist in a single step:</p> <pre><code>j = Job.objects.create(...)\nassert j.id is not None\n</code></pre> <p>Third, you can bulk-create from a collection of in-memory resources.  This is far more efficient for creating large numbers of jobs with fewer API round trips:</p> <pre><code>jobs = [Job(**kwargs) for kwargs in job_specs]\n\n# Capture the return value to get created IDs!\njobs = Job.objects.bulk_create(jobs) \nfor job in jobs:\n    assert job.id is not None\n</code></pre> <p><code>bulk_create</code> does not modify objects in place!</p> <p>When passing a list of items into <code>bulk_create()</code>, you must use the returned value to overwrite the input list with the newly-created items.  This is necessary to set the ID on each item as generated by the server.  Otherwise, the created items will have <code>id == None</code> and generally behave like objects that have never been saved to the API.</p>"},{"location":"user-guide/api/#updating","title":"Updating","text":"<p>If we change some fields on an existing instance, we can update it by calling <code>save()</code>.  The Python API is aware that if the resource <code>id</code> is set, you mean to update an existing resource rather than create a new one.</p> <pre><code>job.state = \"RESTART_READY\"\njob.save()\n</code></pre> <p>If want to load recent changes to an in-memory resource:</p> <pre><code># Re-fetch the Job &amp; update fields:\njob.refresh_from_db()\n</code></pre> <p>We can apply the same change to every item matching a particular <code>Query</code>:</p> <pre><code># Select all FAILED jobs, change num_nodes=1, and mark for retry\nJob.objects.filter(state=\"FAILED\").update(num_nodes=1, state=\"RESTART_READY\")\n</code></pre> <p>If we need to perform a large list of different updates, we can pass a list of mutated instances:</p> <pre><code># Mutated jobs to be updated, each in their own way:\nJob.objects.bulk_update(modified_jobs)\n</code></pre>"},{"location":"user-guide/api/#deleting","title":"Deleting","text":"<p>We can delete individual resources:</p> <pre><code>job.delete()\nassert job.id is None\n</code></pre> <p>Or we can bulk-delete querysets:</p> <pre><code>Job.objects.filter(state=\"FAILED\").delete()\n</code></pre>"},{"location":"user-guide/api/#querying","title":"Querying","text":"<p>Each class has a manager (for instance <code>Job.objects</code> is a <code>JobManager</code>) which generates <code>Query</code> objects.</p>"},{"location":"user-guide/api/#all","title":"All","text":"<p>We can start with a query returning all items with <code>all()</code></p> <pre><code>for job in Job.objects.all():\n    print(job)\n</code></pre>"},{"location":"user-guide/api/#filter","title":"Filter","text":"<p>We can chain queries one-after-another by providing additional sets of filter parameters:</p> <pre><code># These 3 chained queries...\nall_jobs = Job.objects.all()\nfoo_jobs = all_jobs.filter(tags={\"experiment\": \"foo\"})\nfailed_foo_jobs = foo_jobs.filter(state=\"FAILED\")\n\n# ...are equivalent to this 1 query:\nfailed_foo_jobs = Job.objects.filter(\n    tags={\"experiment\": \"foo\"},\n    state=\"FAILED\"\n)\n</code></pre>"},{"location":"user-guide/api/#order-by-and-slicing","title":"Order By and Slicing","text":"<p>When there are hundreds of thousands of Jobs matching your query, it makes sense to order on some criterion and take chunks with the slicing operator:</p> <pre><code># 1000 most recent failures\nJob.objects.filter(state=\"FAILED\").order_by(\"-last_update\")[0:1000]\n</code></pre> <p>Under the hood, the <code>[0:1000]</code> slice operation adds <code>limit</code> and <code>offset</code> to the HTTP query parameters, generating an efficient request that does not fetch more data than you asked for!</p>"},{"location":"user-guide/api/#get","title":"Get","text":"<p>If our query should return exactly one object, we can use <code>get()</code> instead of <code>filter()</code> to return the object directly.  This method raises a model-scoped <code>DoesNotExist</code> error if the query returned 0 items, or <code>MultipleObjectsReturned</code> if more than 1 item was found.  We can catch these errors that arise when when exactly one unique object is expected:</p> <pre><code>try:\n    h2o_job = Job.objects.get(tags={\"system\": \"H2O\"})\nexcept Job.DoesNotExist:\n    print(\"There is no finished H2O job!\")\nexcept Job.MultipleObjectsReturned:\n    print(\"There is more than one finished H2O job!\")\n</code></pre>"},{"location":"user-guide/api/#count","title":"Count","text":"<p>We can use <code>count()</code> to fetch the number of items matching a query, without actually fetching the list of items.  This can be useful to quickly tally up large numbers of <code>Jobs</code>:</p> <pre><code>for state in [\"RUNNING\", \"JOB_FINISHED\"]:\n    count = Job.objects.filter(state=state).count()\n    print(state, count)\n</code></pre>"},{"location":"user-guide/api/#first","title":"First","text":"<p>To grab the first item from a query we can use either syntax:</p> <pre><code>one = Job.objects.filter(state=\"RUNNING\")[0]\n\nsame_thing = Job.objects.filter(state=\"RUNNING\").first()\n</code></pre>"},{"location":"user-guide/api/#lazy-query-evaluation","title":"Lazy Query Evaluation","text":"<p>We can build queries with <code>filter</code>, chain them together, add <code>order_by</code> clauses, apply <code>[start:end]</code> slices, and store these queries in Python variables. None of these actions fetches data because queries are lazily evaluated.</p> <pre><code># This doesn't fetch any Jobs yet:\nfailed_jobs = Job.objects.filter(state=\"FAILED\")\n</code></pre> <p>When a query is evaluated by iteration or some other method, its result is cached, so that repeated iterations over the same query variable do not trigger redundant requests.  </p> <pre><code># This triggers the HTTP request:\nfor job in failed_jobs:\n    print(job)\n\n# This re-uses the cached result:\nfor job in failed_jobs:\n    print(\"Again!\", job)\n\n# We can explicitly force iteration and store the result:\nrunning_jobs = list(Job.objects.filter(state=\"RUNNING\"))\n</code></pre>"},{"location":"user-guide/api/#length","title":"Length","text":"<p>Evaluating the <code>len(query)</code> forces fetching the result set and returns its length.  If you only want the count without fetching the items, its more efficient to use the <code>count()</code> method mentioned above.</p>"},{"location":"user-guide/api/#boolean-value","title":"Boolean Value","text":"<p>Evaluating the query as a Boolean expression (e.g. in an if statement like <code>if query:</code>) also triggers evaluation, and the query evaluates to <code>True</code> if there is at least one object in the result set; it's <code>False</code> otherwise.</p>"},{"location":"user-guide/appdef/","title":"Defining Applications","text":""},{"location":"user-guide/appdef/#registering-applicationdefinitions","title":"Registering <code>ApplicationDefinitions</code>","text":"<p>Once you have a Site, the next step is to define the applications that Balsam may run. Each Site is linked to a set of <code>ApplicationDefinition</code> Python classes: check the Getting Started tutorial to see a quick example of this in action. You can create and register <code>ApplicationDefinition</code> subclasses from any Python session.  For instance, Balsam supports interactive workflows in Jupyter notebooks, or workflows driven by any other Python software.</p> <p>At a minimum, <code>ApplicationDefinitions</code> must declare the <code>site</code> and a <code>command_template</code> for a shell command:</p> <pre><code>from balsam.api import ApplicationDefinition\n\nclass Sleeper(ApplicationDefinition):\n    site = \"polaris\"\n    command_template = 'sleep {{ sleeptime }} &amp;&amp; echo goodbye'\n\nSleeper.sync()\n</code></pre> <p>The <code>site</code> attribute must be present on each <code>ApplicationDefinition</code> subclass to identify where the app runs.  The <code>site</code> can take any of the following types unambiguously specifying a Site:</p> <ul> <li>Site name (uniquely assigned during <code>balsam site init</code>, like <code>\"polaris\"</code>)</li> <li>Site ID (e.g. <code>142</code>)</li> <li><code>Site</code> object (fetched from the API)</li> </ul> <p>The <code>ApplicationDefinition</code> is uniquely identified by its class name and site.   In the above example, we defined the <code>Sleeper</code> application at the <code>polaris</code> Site.  When <code>Sleeper.sync()</code> is called, the Python class and associated metadata is serialized and shipped to the Balsam web API. The <code>Sleeper</code> <code>ApplicationDefinition</code> is thereafter linked to the Site named <code>polaris</code>, and workflows can proceed to submit <code>Sleeper</code> Jobs to <code>polaris</code> from anywhere!</p> <p><code>ApplicationDefinitions</code> must be named uniquely!</p> <p>If another Python session syncs a different <code>Sleeper</code> class belonging to the same Site, the previous application will be overwritten!  This is because apps are uniquely resolved by the pair (<code>site</code>, <code>class_name</code>).  This is typically the desired behavior: simply running <code>sync()</code>  will ensure that Balsam applications stay up-to-date with your source code.  However, inadvertent name collisions can cause unexpected results when you overwrite the implementation of an existing <code>ApplicationDefinition</code>.</p>"},{"location":"user-guide/appdef/#the-submit-shortcut","title":"The <code>submit()</code> shortcut","text":"<p>To run an application, we then submit a Job to invoke the command with specific resources and parameters. The <code>submit()</code> class method provides a convenient syntax to combine the <code>sync</code> initialization step with job submission in a single call:</p> <pre><code># Implicitly sync (updates or creates the App) and submit a Job:\njob = Sleeper.submit(workdir=\"test/sleep_3\", sleeptime=3)\n</code></pre> <p>This shorthand syntax is completely equivalent to the following: <pre><code>from balsam.api import Job\n\nSleeper.sync()\n\njob = Job(workdir=\"test/sleep_3\", app_id=Sleeper, parameters={\"sleeptime\": 3})\njob.save()\n</code></pre></p> <p>It's more efficient to use bulk-creation to submit large numbers of Jobs in a single network call.  This is possible by passing the special keyword argument <code>save=False</code>:</p> <pre><code>jobs = [\n    Sleeper.submit(workdir=f\"test/{i}\", sleeptime=3, save=False)\n    for i in range(100)\n]\njobs = Job.objects.bulk_create(jobs)\n</code></pre> <p>Besides the special <code>save</code> kwarg, The <code>submit()</code> method has the same signature as the <code>Job()</code> constructor which is covered in-depth on the next page.</p>"},{"location":"user-guide/appdef/#python-applications","title":"Python Applications","text":"<p>Besides running shell commands, Balsam can run Python applications directly on the compute nodes.  This paradigm significantly cuts down boilerplate and reduces the need for creating \"entry point\" scripts.</p> <p>Instead of using <code>command_template</code>, the <code>ApplicationDefinition</code> can simply define a <code>run()</code> method that will be launched using the exact same rules as ordinary shell applications.</p> <pre><code>class Adder(ApplicationDefinition):\n    site = \"polaris\"\n\n    def run(self, x, y):\n        return x + y\n\njob = Adder.submit(\"test/5plus5\", x=5, y=5)\nassert job.result() == 10  # This will block until a BatchJob starts\n</code></pre> <p><code>run</code> is an instance method and should take <code>self</code> as the first argument.  Additional positional or keyword arguments can be supplied as well.  When submitting <code>Jobs</code>, the parameters are serialized (under the hood <code>job.parameters = {\"x\": 5, \"y\": 5}</code> is converted to a Base64-encoded byte string with <code>dill</code> and stored as part of the <code>Job</code> in the Balsam web API.)</p> <p>The submitted <code>Jobs</code> behave partially like <code>concurrent.futures.Future</code> objects: namely, the return value or Exception raised by <code>run()</code> will propagate to the <code>result()</code> method.  Refer to the Jobs documentation for more details on these <code>Future</code>-like APIs.</p>"},{"location":"user-guide/appdef/#python-app-capabilities-and-limitations","title":"Python App Capabilities and Limitations","text":"<p>Python <code>run()</code> function-based <code>ApplicationDefinitions</code> enjoy all the same lifecycle hooks and flexible resource launching capabilities as ordinary <code>ApplicationDefinitions</code>.  For instance, your Balsam apps can directly call into <code>mpi4py</code> code and be launched onto multiple compute nodes:</p> <pre><code>import numpy as np\n\n\nclass NumpyPlusMPI(ApplicationDefinition):\n    site = \"polaris\"\n\n    def run(self, *dims):\n        from mpi4py import MPI\n        if MPI.COMM_WORLD.Get_rank() == 0:\n            return np.random.rand(*dims)\n\n\njob = NumpyPlusMPI.submit(\n    workdir=\"test/parallel\", \n    dims=[5],\n    ranks_per_node=4,\n)\n</code></pre> <p>The <code>run</code> function can generally refer to imported modules and objects from other namespaces, as long as they are importable in the Balsam Site's Python environment.  This is a crucial constraint to understand when writing <code>ApplicationDefinitions</code>: all import statements are essentially replayed in the Balsam Site environment when the <code>ApplicationDefinition</code> is loaded and de-serialized.  This will fail if any any object referenced by the <code>ApplicationDefinition</code> cannot be imported from <code>sys.path</code>!</p> <p>Import statements will replay on the Balsam Site</p> <p>The issues above are easily avoided by ensuring that your codes and their dependencies are installed in the Python virtual environment where the Balsam Site runs.  </p> <p>Just remember that the Balsam app serializer does not recurse and ship other modules over the wire. Virtually any import statement referenced by the <code>ApplicationDefintion</code> must work on both sides.</p> <p>The same constraint holds true when the <code>ApplicationDefinitions</code> themselves are located in an imported module.  For example, if your Balsam code is packaged in <code>my_science_package</code>, that module/package must be installed in the Site environment where the <code>ApplicationDefinition</code> is synced to.</p> <pre><code># This import line must work on polaris...\nfrom my_science_package import setup_workflow\n\n# If this method syncs apps to polaris:\nsetup_workflow(site=\"polaris\")\n</code></pre> <p>Technical Details</p> <p>Under the hood, all <code>ApplicationDefinitions</code>, <code>Job</code> parameters, return values, and exceptions are serialized and deserialized using <code>dill</code> with the <code>recurse=True</code> option.</p> <p>Besides the issue of software dependencies, the following additional limitations should be kept in mind when writing <code>ApplicationDefinitions</code>:</p> <ul> <li>Do not use any <code>multiprocessing.Process</code>. You can spawn background <code>Thread</code> tasks, but <code>Process</code> workers will attempt to communicate using <code>pickle</code> in a fashion that is incompatible with the serialization scheme used by Balsam.</li> <li>Do not use the <code>super()</code> syntax anywhere in the <code>ApplicationDefinition</code>.  This is a known issue with <code>dill</code>.  If you are writing your own class hierarchies, you can always get around this by referencing the parent class directly.</li> <li>Avoid referring to <code>balsam.api</code> in the <code>ApplicationDefinition</code>. Instead, you will need to manually initialize the client to fetch resources as follows:     <pre><code>from balsam.config import SiteConfig\nclient = SiteConfig().client  # Loads a fresh RESTClient\nJob = client.Job  # Use just like balsam.api.Job\n</code></pre></li> <li><code>ApplicationDefinitions</code>, parameters, return values, and exceptions should be lean.  Do not rely on the Balsam API to move large quantities of data (instead, the Transfer API is designed for easy interoperability with out-of-band transfer methods like Globus).  Balsam imposes limits on the order of 100KB for each serialized entity.</li> </ul>"},{"location":"user-guide/appdef/#listing-and-refreshing-applications","title":"Listing and Refreshing Applications","text":"<p>You can check the Apps registered at a site using the CLI:</p> <pre><code># List apps across all Sites\n$ balsam app ls --site=all\n</code></pre> <p>Restart Sites to Reload <code>ApplicationDefinitions</code></p> <p>A new <code>ApplicationDefinition</code> will be automatically loaded by a running Site agent or launcher BatchJob.  However, if you modify an existing app while it is loaded in a running Site or launcher, the change will not  propagate!  You must remember to run <code>balsam site sync</code> to refresh the loaded apps in a running Site.</p>"},{"location":"user-guide/appdef/#loading-existing-applicationdefinitions","title":"Loading Existing <code>ApplicationDefinitions</code>","text":"<p>You don't need a handle on the <code>ApplicationDefinition</code> source code to submit <code>Jobs</code> with it.  Instead, the <code>app_id</code> argument to <code>Job()</code> can take an application by  name (string), ID (integer), or reference to a loaded <code>ApplicationDefinition</code>. For example, you can load the set of applications registered at a given Site as follows:</p> <pre><code>apps_by_name = ApplicationDefinition.load_by_site(\"polaris\")\nSleeper = apps_by_name[\"Sleeper\"]\n</code></pre> <p><code>ApplicationDefinitions</code> can then be used from this dictionary as if you had defined them in the current Python session.</p>"},{"location":"user-guide/appdef/#writing-applicationdefinitions","title":"Writing ApplicationDefinitions","text":"<p>At their simplest, <code>ApplicationDefinitions</code> provide a declarative template for a shell command and its adjustable parameters. Alternatively, they define a Python <code>run()</code> function that takes arbitrary inputs. To run an application, we submit a Job that provides values for these parameters. </p> <p>Importantly, we do not specify how the application is launched (<code>mpiexec</code>) or its CPU/GPU resources in the <code>ApplicationDefinition</code>.  Instead, Balsam takes care of managing resources and building the command lines to efficiently launch our Jobs.</p> <p>Besides the fundamental <code>site</code>, <code>command_template</code>, and <code>run</code> attributes discussed above, <code>ApplicationDefinitions</code> provide other special attributes and methods that we can override to build more complex and useful workflow components.</p>"},{"location":"user-guide/appdef/#the-class-path","title":"The Class Path","text":"<p>Balsam Apps are uniquely identified by:</p> <ol> <li>The <code>Site</code> that they belong to</li> <li>Their <code>ApplicationDefinition</code> class <code>__name__</code></li> </ol> <p>For instance, the <code>Sleeper</code> application we defined above in <code>test.py</code> has a <code>name</code> of <code>Sleeper</code>. We use this to uniquely identify each <code>ApplicationDefinition</code> class later on.</p>"},{"location":"user-guide/appdef/#the-description","title":"The Description","text":"<p>The docstring that follows the <code>class</code> statement is captured by Balsam  and stored as a <code>description</code> with the REST API.  This is purely human-readable text that can be displayed in your App catalog.</p> <pre><code>class MySimulation(ApplicationDefinition):\n    \"\"\"\n    Some description of the app goes here\n    \"\"\"\n</code></pre>"},{"location":"user-guide/appdef/#the-site-identifier","title":"The Site Identifier","text":"<p>The <code>site</code> attribute is required on all <code>ApplicationDefinitions</code> and it must unambiguously refer to one of your existing Sites.  This class attribute can be a string (site name), integer (site ID), or a <code>Site</code> object loaded from the Balsam SDK.</p> <pre><code>class MySimulation(ApplicationDefinition):\n    \"\"\"\n    Some description of the app goes here\n    \"\"\"\n    site = \"polaris\"\n</code></pre>"},{"location":"user-guide/appdef/#environment-variables","title":"Environment Variables","text":"<p>The <code>environment_variables</code> attribute should be a <code>Dict[str, str]</code> mapping environment variable names to values.  This is useful for constant environment variables that do not vary across runs. This environment is merged with the environment established in the job  template.</p> <pre><code>class MySimulation(ApplicationDefinition):\n    \"\"\"\n    Some description of the app goes here\n    \"\"\"\n    site = \"polaris\"\n    environment_variables = {\n        \"HDF5_USE_FILE_LOCKING\": \"FALSE\",\n    }\n</code></pre>"},{"location":"user-guide/appdef/#command-template","title":"Command Template","text":"<p>As we have seen, <code>ApplicationDefinitions</code> must contain either a <code>command_template</code> or a <code>run()</code> method.  These are mutually exclusive: you must set one or the other. The <code>command_template</code> is interpreted as a Jinja2 template; therefore, parameters must be enclosed in double-curly braces.</p> <pre><code>class MySimulation(ApplicationDefinition):\n    \"\"\"\n    Some description of the app goes here\n    \"\"\"\n    site = \"polaris\"\n    environment_variables = {\n        \"HDF5_USE_FILE_LOCKING\": \"FALSE\",\n    }\n    command_template = \"/path/to/simulation.exe -inp {{ input_filename }}\"\n</code></pre> <p>By default, all app parameters are required parameters: it is an error to omit any parameter named in the template.  We can change this behavior below.</p>"},{"location":"user-guide/appdef/#the-run-function","title":"The <code>run</code> function","text":"<p>When the <code>ApplicationDefinition</code> contains a <code>run()</code> method, this function is launched onto compute resources using the parameters set on the corresponding <code>Job</code>.</p> <pre><code>import numpy as np\n\nclass VecNorm(ApplicationDefinition):\n    site = \"polaris\"\n\n    def run(self, vec):\n        return np.linalg.norm(vec)\n</code></pre>"},{"location":"user-guide/appdef/#python-executable","title":"Python executable","text":"<p>When using a <code>run()</code> function, it is important that the execution-side Python environment has the necessary dependencies installed. The optional class attribute <code>python_exe</code> defaults to <code>sys.executable</code> and should not be changed if the app runs in the same environment Balsam is installed in. </p> <p>You should override <code>python_exe</code> if you wish to invoke the <code>run</code> function using a different Python environment from the one in which Balsam is installed.  This setting has no effect for <code>command_template</code> apps.</p> <pre><code>import numpy as np\n\nclass VecNorm(ApplicationDefinition):\n    site = \"polaris\"\n    python_exe = \"/path/to/bin/python3.8\"\n\n    def run(self, vec):\n        import numpy as np # Loaded from `python_exe`\n        return np.linalg.norm(vec)\n</code></pre>"},{"location":"user-guide/appdef/#parameter-spec","title":"Parameter Spec","text":"<p>Maybe we want to have some optional parameters in the <code>command_template</code>, which take on a default value in the absence of a value specified in the Job. We can do this by providing the <code>parameters</code> dictionary:</p> <pre><code>class MySimulation(ApplicationDefinition):\n    \"\"\"\n    Some description of the app goes here\n    \"\"\"\n    site = \"polaris\"\n    environment_variables = {\n        \"HDF5_USE_FILE_LOCKING\": \"FALSE\",\n    }\n    command_template = \"/path/to/sim.exe --mode {{ mode }} -inp {{ input_filename }}\"\n    parameters = {\n        \"input_filename\": {\"required\": True},\n        \"mode\": {\n            \"required\": False, \n            \"default\": \"explore\", \n            \"help\": \"The simulation mode (default: explore)\",\n        }\n    }\n</code></pre> <p>Notice that parameters are either required, in which case it doesn't make sense to have a default value, or not. If a parameter's <code>required</code> value is <code>False</code>, you must provide a <code>default</code> value that is used when the parameter is not passed.</p> <p>The <code>help</code> field is another optional, human-readable field, to assist with App curation in the Web interface.</p> <p>Valid Python Identifiers</p> <p>App parameters can only contain valid Python identifiers, so names with <code>-</code>, for instance, will be rejected when you  attempt to run <code>balsam app sync</code>.</p>"},{"location":"user-guide/appdef/#transfer-slots","title":"Transfer Slots","text":"<p>A core feature of Balsam, described in more detail in the Data Transfers section, is the ability to write distributed workflows, where data products move between Sites, and Jobs can be triggered when data arrives at its destination.</p> <p>We create this behavior starting at the <code>ApplicationDefinition</code> level, by defining Transfer Slots for data that needs to be staged in before or staged out after execution.  You can think of the Job workdir as an ephemeral sandbox where data arrives, computation happens, and then results are staged out to a more accessible location for further analysis.</p> <p>Each <code>ApplicationDefinition</code> may declare a <code>transfers</code> dictionary, where each string key names a Transfer Slot.</p> <pre><code>class MySimulation(ApplicationDefinition):\n    transfers = {\n        \"input_file\": {\n            \"required\": True,\n            \"direction\": \"in\",\n            \"local_path\": \"input.nw\",\n            \"description\": \"Input Deck\",\n            \"recursive\": False,\n        },\n        \"result\": {\n            \"required\": True,\n            \"direction\": \"out\",\n            \"local_path\": \"job.out\",\n            \"description\": \"Calculation stdout\",\n            \"recursive\": False\n        },\n    },\n</code></pre> <p>In order to fill the slots, each <code>Job</code> invoking this application must then provide concrete URIs of the external files:</p> <pre><code>Job.objects.create(\n    workdir=\"ensemble/1\",\n    app_name=\"sim.MySimulation\",\n    transfers={\n        # Using 'laptop' alias defined in settings.yml\n        \"input_file\": \"laptop:/path/to/input.dat\",\n        \"result\": \"laptop:/path/to/output.json\",\n    },\n)\n</code></pre> <p>Transfer slots with <code>required=False</code> are optional when creating Jobs. The <code>direction</code> key must contain the value <code>\"in\"</code> or <code>\"out\"</code> for stage-in and stage-out, respectively. The <code>description</code> is an optional, human-readable parameter to assist in App curation. The <code>recursive</code> flag should be <code>True</code> for directory transfers; otherwise, the transfer is treated as a single file. </p> <p>Finally, <code>local_path</code> must always be given relative to the Job workdir.  When <code>direction=in</code>, the <code>local_path</code> refers to the transfer destination.  When <code>direction=out</code>, the <code>local_path</code> refers to the transfer source.  This <code>local_path</code> behavior encourages a pattern where files in the working directory are always named identically, and only the remote sources and destinations vary. If you need to stage-in remote files without renaming them, a <code>local_path</code> value of <code>.</code> can be used.</p> <p>After running <code>balsam app sync</code>, the command <code>balsam app ls --verbose</code> will show any transfer slots registered for each of your apps.</p>"},{"location":"user-guide/appdef/#cleanup-files","title":"Cleanup Files","text":"<p>In long-running data-intensive workflows, a Balsam site may exhaust its HPC storage allocation and trigger disk quota errors.  To avoid this problem, valuable data products should be packaged and staged out, while   intermediate files are periodically deleted to free storage space.  The Site <code>file_cleaner</code> service can be enabled in <code>settings.yml</code> to safely remove files from working directories of finished jobs.  Cleanup does not occur until a job reaches the <code>JOB_FINISHED</code> state, after all stage out tasks have completed.</p> <p>By default, the <code>file_cleaner</code> will not delete anything, even when it has been enabled.  The <code>ApplicationDefinition</code> must also define a list of glob patterns in the <code>cleanup_files</code> attribute, for which matching files will be removed upon job completion.</p> <pre><code>class MySimulation(ApplicationDefinition):\n    \"\"\"\n    Some description of the app goes here\n    \"\"\"\n    site = \"polaris\"\n    environment_variables = {\n        \"HDF5_USE_FILE_LOCKING\": \"FALSE\",\n    }\n    command_template = \"/path/to/simulation.exe -inp {{ input_filename }}\"\n    cleanup_files = [\"*.hdf\", \"*.imm\", \"*.h5\"]\n</code></pre> <p>Cleanup occurs once for each finished Job and reads the list of deletion patterns from the <code>cleanup_files</code> attribute in the <code>ApplicationDefinition</code>.</p>"},{"location":"user-guide/appdef/#job-lifecycle-hooks","title":"Job Lifecycle Hooks","text":"<p>The <code>ApplicationDefinition</code> class provides several hooks into stages of the Balsam Job lifecycle, in the form of overridable methods on the class.  These methods are called by the Balsam Site as it handles your Jobs, advancing them from <code>CREATED</code> to <code>JOB_FINISHED</code> through a series of state transitions.</p> <p>To be more specific, an instance of the <code>ApplicationDefinition</code> class is created for each <code>Job</code> as it undergoes processing. The hooks are called as ordinary instance methods, where <code>self</code> refers to an <code>ApplicationDefinition</code> object handling a particular <code>Job</code>.  The current <code>Job</code> can be accessed via the <code>self.job</code> attribute (see examples below).  Of course, you may define any additional methods on the class and access them as usual.</p> <p><code>ApplicationDefinitions</code> are not persistent!</p> <p><code>ApplicationDefinition</code> instances are created and torn down after each invocation of a hook for a particular Job.  This is because they might execute days or weeks apart on different physical hosts.  Therefore, any data that you set on the <code>self</code> object within the hook will not persist. Instead, hooks can persist arbitrary JSON-serializable data on the <code>Job</code> object itself via <code>self.job.data</code>.</p> <p>Hook methods are always executed in the current <code>Job</code>'s working directory with stdout/stderr routed into the file <code>balsam.log</code>.  All of the methods described below are optional: the default implementation is essentially a no-op that moves the <code>Job</code> state forward.  However, if you do choose to override a lifecycle hook, it is your responsibility to set the <code>Job</code> state appropriately (e.g. you must write <code>self.job.state = \"PREPROCESSED\"</code> in the <code>preprocess()</code> function).  The reason for this is that hooks may choose to retry or fail a particular state transition; the <code>ApplicationDefinition</code> should be the explicit source of truth on these possible actions.</p>"},{"location":"user-guide/appdef/#the-preprocess-hook","title":"The Preprocess Hook","text":"<p>The <code>preprocess</code> method advances jobs from <code>STAGED_IN</code> to <code>PREPROCESSED</code>.  This represents an opportunity to run lightweight or I/O-bound code on the login node after any data for a Job has been staged in, and before the application begins executing. This runs in the <code>processing</code> service on the host where the Site Agent is running.</p> <p>In the following example, <code>preprocess</code> is used to read some user-defined data from the <code>Job</code> object, attempt to generate an input file, and advance the job state only if the generated input was valid.</p> <pre><code>class MySimulation(ApplicationDefinition):\n\n    def preprocess(self):\n        # Can read anything from self.job.data\n        coordinates = self.job.data[\"input_coords\"]\n\n        # Run arbitrary methods defined on the class:\n        success = self.generate_input(coordinates)\n\n        # Advance the job state\n        if success:\n            # Ready to run\n            self.job.state = \"PREPROCESSED\"\n        else:\n            # Fail the job and attach searchable data\n            # to the failure event\n            self.job.state = \"FAILED\"\n            self.job.state_data = {\"error\": \"Preproc got bad coordinates\"}\n</code></pre>"},{"location":"user-guide/appdef/#the-shell-preamble","title":"The Shell Preamble","text":"<p>The <code>shell_preamble</code> method can return a multi-line string or a list of strings, which are executed in an ephemeral <code>bash</code> shell immediately preceding the application launch command.  This hook directly affects the environment of the <code>mpirun</code> (or equivalent) command used to launch each Job; therefore, it is appropriate for loading modules or exporting environment variables in an App- or Job-specific manner. Unlike <code>preprocess</code>, this hook is executed by the launcher (pilot job) on the application launch node.  </p> <pre><code>class MySimulation(ApplicationDefinition):\n\n    def shell_preamble(self):\n        return f'''\n        module load conda/tensorflow\n        export FOO={self.job.data[\"env_vars\"][\"foo\"]}\n        '''\n</code></pre>"},{"location":"user-guide/appdef/#the-postprocess-hook","title":"The Postprocess Hook","text":"<p>The <code>postprocess</code> hook is exactly like the <code>preprocess</code> hook, except that it runs after Jobs have successfully executed.  In Balsam a \"successful execution\" simply means the application command return code was <code>0</code>, and the job is advanced by the launcher from <code>RUNNING</code> to <code>RUN_DONE</code>. Some common patterns in the <code>postprocess</code> hook include: </p> <ul> <li>parsing output files</li> <li>summarizing/archiving useful data to be staged out</li> <li>persisting data on the <code>job.data</code> attribute</li> <li>dynamically creating additional <code>Jobs</code> to continue the workflow</li> </ul> <p>Upon successful postprocessing, the job state should be advanced to <code>POSTPROCESSED</code>.  However, a return code of 0 does not necessarily imply a successful run. The method may therefore choose to set a job as <code>FAILED</code> (to halt further processing) or <code>RESTART_READY</code> (to run again, perhaps after changing some input).</p> <pre><code>class MySimulation(ApplicationDefinition):\n\n    def postprocess(self):\n        with open(\"out.hdf\") as fp:\n            # Call your own result parser:\n            results = self.parse_results(fp)\n\n        if self.is_converged(results):\n            self.job.state = \"POSTPROCESSED\"\n        else:\n            # Call your own input file fixer:\n            self.fix_input()\n            self.job.state = \"RESTART_READY\"\n</code></pre>"},{"location":"user-guide/appdef/#timeout-handler","title":"Timeout Handler","text":"<p>We have just seen how the <code>postprocess</code> hook handles the return code <code>0</code> scenario by moving jobs from <code>RUN_DONE</code> to <code>POSTPROCESSED</code>.  There are two less happy scenarios that Balsam handles:</p> <ol> <li>The launcher wallclock time expired and the Job was terminated while still running.  The launcher marks the job state as <code>RUN_TIMEOUT</code>.</li> <li>The application finished with a nonzero exit code. This is interpreted by the launcher as an error, and the job state is set to <code>RUN_ERROR</code>.</li> </ol> <p>The <code>handle_timeout</code> hook gives us an opportunity to manage timed-out jobs in the <code>RUN_TIMEOUT</code> state. The default Balsam action is to immediately mark the timed out job as <code>RESTART_READY</code>: it is simply eligible to run again as soon as resources are available.  If you wish to fail the job or tweak inputs before running again, this is the right place to do it.  </p> <p>In this example, we choose to mark the timed out job as <code>FAILED</code> but dynamically generate a follow-up job with related parameters.</p> <pre><code>from balsam.api import Job\n\nclass MySimulation(ApplicationDefinition):\n\n    def handle_timeout(self):\n        # Sorry, not retrying slow runs:\n        self.job.state = \"FAILED\"\n        self.job.state_data = {\"reason\": \"Job Timed out\"}\n\n        # Create another, faster run:\n        new_job_params = self.next_run_kwargs()\n        Job.objects.create(**new_job_params)\n</code></pre>"},{"location":"user-guide/appdef/#error-handler","title":"Error Handler","text":"<p>The <code>handle_error</code> hook handles the second scenario listed in the previous section: when the job terminates with a nonzero exit code.  If you can fix the error and try again, set the job state to <code>RESTART_READY</code>; otherwise, the default implementation simply fails jobs that encountered a <code>RUN_ERROR</code> state.</p> <p>The following example calls some user-defined <code>fix_inputs()</code> to retry a failed run up to three times before declaring the job as <code>FAILED</code>.</p> <pre><code>class MySimulation(ApplicationDefinition):\n\n    def handle_error(self):\n        dat = self.job.data\n        retry_count = dat.get(\"retry_count\", 0)\n\n        if retry_count &lt;= 3:\n            self.fix_inputs()\n            self.job.state = \"RESTART_READY\"\n            self.job.data = {**dat, \"retry_count\": retry_count+1}\n        else:\n            self.job.state = \"FAILED\"\n            self.job.state_data = {\"reason\": \"Exceeded maximum retries\"}\n</code></pre> <p>Be careful when updating <code>job.data</code>!</p> <p>Notice in the example above that we did not simply update <code>self.job.data[\"retry_count\"]</code>, even though that's the only value that changed.  Instead, we created a new dictionary merging the existing contents of <code>data</code> with the incremented value for <code>retry_count</code>. If we had attempted the former method, <code>job.data</code> would not have been updated.</p> <p>This is a subtle consequence of the Balsam Python API, which tracks mutated data on the <code>Job</code> object whenever a new value is assigned to one of the object's fields. This works great for immutable values, but unfortunately, updates to mutable fields (like appending to a list or setting a new key:value pair on a dictionary)  are not currently intercepted.</p> <p>The Balsam <code>processing</code> service that runs these lifecycle hooks inspects mutations on each <code>Job</code> and propagates efficient bulk-updates to the REST API.</p>"},{"location":"user-guide/batchjob/","title":"Scheduling Launchers","text":"<p>Unless Auto Scaling is enabled, Jobs do not automatically run in Balsam.  After the Site agent completes data staging and preprocessing for a Job, it waits in the <code>PREPROCESSED</code> state until a launcher (pilot job) submitted to the HPC batch scheduler takes over.</p> <p>Launchers run independently of the Site agent, continuously fetching and  executing runnable Jobs on the available compute resources.  They track occupancy of the allocated CPUs/GPUs while launching Jobs with the requested resources. Because launchers acquire Jobs on-the-fly, you can submit Jobs to any system, and they will execute in real-time on an existing allocation!</p> <p>The Balsam launcher thus handles the compute-intensive core of the Job lifecycle: from <code>PREPROCESSED</code> to <code>RUNNING</code> to <code>RUN_DONE</code>. As users, we need only submit a <code>BatchJob</code> to any one of our Sites.  The <code>BatchJob</code> represents an HPC resource allocation as a fixed block of node-hours.  Sites will handle new <code>BatchJobs</code> by generating the script to run a launcher and submitting it to the local HPC scheduler.</p>"},{"location":"user-guide/batchjob/#using-the-cli","title":"Using the CLI","text":"<p>If we are inside a Site directory, we can submit a <code>BatchJob</code> to that Site from the CLI:</p> <pre><code>$ balsam queue submit -q QUEUE -A PROJECT -n 128 -t 70 -j mpi \n</code></pre> <p>The CLI works remotely, too: we just need to target a specific <code>--site</code> so Balsam knows where you want things to run:</p> <pre><code>$ balsam queue submit --site=my-site -q QUEUE -A PROJECT -n 128 -t 70 -j mpi \n</code></pre> <p>Balsam will perform the appropriate submission to the underlying HPC scheduler and synchronize your <code>BatchJobs</code> with the local scheduler state. Therefore, instead of checking queues locally (e.g. <code>qstat</code>), we can check on <code>BatchJobs</code> across all of our Sites with a simple command:</p> <pre><code>$ balsam queue ls --site=all\n</code></pre> <p>Systems without a batch scheduler</p> <p>Use <code>-n 1 -q local -A local</code> when creating <code>BatchJobs</code> at generic MacOS/Linux Sites without a real batch scheduler or multiple nodes. The OS process manager takes the place of the resource manager, but everything else looks the same!</p>"},{"location":"user-guide/batchjob/#selecting-a-launcher-job-mode","title":"Selecting a Launcher Job Mode","text":"<p>All of the <code>queue submit</code> options pass through to the usual scheduler interface (like <code>sbatch</code> or <code>qsub</code>), except for the <code>-j/--job-mode</code>  flag, which may be either <code>mpi</code> or <code>serial</code>. These refer to the launcher job modes, which determines the pilot job implementation that will actually run.</p> <ul> <li><code>mpi</code> mode is the most flexible and should be preferred unless you have a particularly extreme throughput requirement or need to use one of the workarounds offered by <code>serial</code> mode.  The <code>mpi</code> launcher runs on the head-node of the <code>BatchJob</code> and executes each job using the system's MPI launch command (e.g. <code>srun</code> or <code>mpirun</code>).</li> <li><code>serial</code> mode only handles single-process (non-distributed memory) Jobs that run within a single compute node.  Higher throughput of fine-grained tasks (e.g. millions of single-core tasks) is achieved by running a worker process on each compute node and fanning out cached <code>Jobs</code> acquired from the REST API.</li> </ul> <p>Both launcher modes can simultaneously execute multiple applications per node, as long as the underlying HPC system provides support.</p> <p>You can submit multiple BatchJobs to a Site</p> <p>Balsam launchers cooperatively divide and conquer the runnable Jobs at a Site.  You may therefore choose between queueing up fewer large BatchJobs or several smaller BatchJobs simultaneously.  On a busy HPC cluster, smaller BatchJobs can get through the queues faster and improve overall throughput.</p>"},{"location":"user-guide/batchjob/#ordering-job-execution","title":"Ordering Job Execution","text":"<p>By default, Balsam will sort jobs that are ready to run first by <code>num_nodes</code>  in acending order, then by <code>node_packing_count</code> in decending order, and finally  by <code>wall_time_min</code> in decending order.  This default behavior will result in  the smallest jobs by node count starting first.</p> <p>There is an alternative sorting model that can be enabled that sorts jobs first  by <code>wall_time_min</code> in decending order, then by <code>num_nodes</code> in decending order,  and finally by <code>node_packing_count</code> in decending order.  This alternative  sorting behavior will start the longest running jobs, as estimated by  <code>wall_time_min</code>, first.  If jobs have no <code>wall_time_min</code> set, it will start  the largest jobs by node count first.  This alternative sorting model can be  enabled for the site by modifying the site's configuration <code>settings.yml</code> file. Under <code>launcher</code>, add this option: <pre><code>sort_by: long_large_first\u00a0# set this to enable alternative sorting model that starts the longest running and largest node count jobs first\n</code></pre> Restart the site after changing <code>settings.yml</code> for the changes to take effect.</p>"},{"location":"user-guide/batchjob/#using-the-api","title":"Using the API","text":"<p>A unique capability of the Balsam Python API is that it allows us programmatically manage HPC resources (via <code>BatchJob</code>) and tasks (via <code>Job</code>) on equal footing. We can submit and monitor <code>Jobs</code> and <code>BatchJobs</code> at any Site with ease, using a single, consistent programming model.</p> <pre><code>from balsam.api import Job, BatchJob\n# Create Jobs:\njob = Job.objects.create(\n    site_name=\"myProject\",\n    app_id=\"SimulationX\",\n    workdir=\"test-runs/foo/1\",\n)\n\n# Or allocate resources:\nBatchJob.objects.create(\n    site_id=job.site_id,\n    num_nodes=1,\n    wall_time_min=20,\n    job_mode=\"mpi\",\n    project=\"datascience\",\n    queue=\"debug\",\n)\n</code></pre> <p>We can query <code>BatchJobs</code> to track how many resources are currently available or waiting in the queue at each Site:</p> <pre><code>queued_nodes = sum(\n    batch_job.num_nodes\n    for batch_job in BatchJob.objects.filter(site_id=123, state=\"queued\")\n)\n</code></pre> <p>Or we can instruct Balsam to cleanly terminate an allocation:</p> <pre><code>BatchJob.objects.filter(scheduler_id=1234).update(state=\"pending_deletion\")\n</code></pre> <p>Jobs running in that <code>BatchJob</code> will be marked <code>RUN_TIMEOUT</code> and handled by their respective Apps' <code>handle_timeout</code> hooks.</p>"},{"location":"user-guide/batchjob/#job-templates-and-specialized-parameters","title":"Job Templates and specialized parameters","text":"<p>Behind the scenes, each <code>BatchJob</code> materializes as a batch job script rendered from the Site's job template. These templates can be customized to support new scheduler flags, load global modules, or perform general pre-execution logic. These templates also accept optional, system-specific parameters that can be passed on the CLI via <code>-x</code> or to the BatchJob <code>optional_params</code> dictionary.</p>"},{"location":"user-guide/batchjob/#restricting-batchjobs-with-tags","title":"Restricting BatchJobs with tags","text":"<p>We strongly encourage the use of descriptive tags to facilitate monitoring Jobs. Another major use of tags is to restrict which Jobs can run in a given BatchJob.</p> <p>The default behavior is that a BatchJob will run as many Jobs as possible: Balsam decides what runs in any given allocation. But perhaps we wish to prioritize a certain group of runs, or deliberately run Jobs in separate partitions as part of a scalability study.  </p> <p>This is easy with the CLI:</p> <pre><code># Only run jobs with tags system=H2O and scale=4\n$ balsam queue submit -n 4 --tag system=H2O --tag scale=4  # ...other args\n</code></pre> <p>Or with the API: <pre><code>BatchJob.objects.create(\n    # ...other kwargs\n    num_nodes=4,\n    filter_tags={\"system\": \"H2O\", \"scale\": \"4\"}\n)\n</code></pre></p>"},{"location":"user-guide/batchjob/#partitioning-batchjobs","title":"Partitioning BatchJobs","text":"<p>By default, a single launcher process manages the entire allocation of compute nodes with a single job mode of either <code>mpi</code> or <code>serial</code>.  In advanced use-cases, we can actually divide a single queue submission/allocation into multiple launcher partitions.</p> <p>Each of this partitions can have its own number of compute nodes, job mode, and filter tags.  This can be useful in different contexts: </p> <ul> <li>Dividing an allocation to run a mixed workload of MPI applications and high-throughput sub-node applications.  </li> <li>Improving scalability to large node counts by parallelizing the job launcher.</li> </ul> <p>We can request that a BatchJob is split into partitions on the CLI. In this example, we split a 128-node allocation into a 2-node MPI launcher (to run some \"leader\" MPI app on 2 nodes), while the remaining 126 nodes are managed by the efficient <code>serial</code> mode launcher for high-throughput.</p> <pre><code>$ balsam queue submit -n 128 -p mpi:2 -p serial:126 # ...other args\n</code></pre> <p>We could also apply tag restrictions to ensure that the right Jobs run in the right partition:</p> <pre><code>$ balsam queue submit -n 128 -p mpi:2:role=leader -p serial:126:role=worker # ...other args\n</code></pre> <p>With the Python API, this looks like: <pre><code>BatchJob.objects.create(\n    # ...other kwargs\n    num_nodes=128,\n    partitions=[\n        {\"job_mode\": \"mpi\", \"num_nodes\": 2, \"filter_tags\": {\"role\": \"leader\"}},\n        {\"job_mode\": \"serial\", \"num_nodes\": 126, \"filter_tags\": {\"role\": \"worker\"}},\n    ]\n)\n</code></pre></p>"},{"location":"user-guide/cli/","title":"The Command Line Interface (CLI)","text":"<p>When installed, Balsam provides a <code>balsam</code> command line tool for convenient, shell-based management of your Sites, Apps, Jobs, and BatchJobs. The CLI comprises recursively-documented subcommands: use the <code>--help</code> flag at  any level to view example usage:</p> <pre><code># See all commands:\n$ balsam --help\n\n# See all job-related commands:\n$ balsam job --help\n\n# See details of CLI job creation:\n$ balsam job create --help\n</code></pre>"},{"location":"user-guide/cli/#the-site-selector","title":"The Site Selector","text":"<p>A key feature of the CLI (and underlying API) is that it works across sites: you might, for example, query and submit jobs to three HPC facilities from a single shell running on your laptop. This raises a namespacing concern: how should the CLI commands target different Sites?</p> <p>To answer this question, the Balsam CLI is somewhat context-aware.  When you are logged into the machine and inside of a Site directory, the current site is automatically inferred.  Thus commands like <code>balsam job ls</code> will filter the visible Jobs to only those in the current Site. Likewise, commands like  <code>balsam job create</code> or <code>balsam queue submit</code> will infer the ID of the Site for which you are trying to add a Job or BatchJob.</p> <p>If you are outside of a Site directory, the CLI instead limits queries to all currently active Sites.  For example, <code>balsam job ls</code> will list Jobs across all your Sites that have an Agent process currently running.</p> <p>We can override this context-dependent behavior by explicitly passing the <code>--site</code> selector argument as follows:</p> <pre><code>$ balsam job ls --site=all # Show jobs at ALL sites\n$ balsam job ls --site=this # Only show jobs at the current site\n$ balsam job ls --site=active # Show jobs at active sites only\n</code></pre> <p>Moreover, the <code>--site</code> selector can provide a comma-separated list of Site IDs or Site Path fragments:</p> <pre><code>$ balsam job ls --site=123,125\n$ balsam job ls --site=myFolder/siteX,siteY,123\n</code></pre> <p>When you are creating a Job or enqueueing a BatchJob, the <code>--site</code> selector must unambiguously narrow down to a single Site. In this case, use a single numeric Site ID (as shown in <code>balsam site ls</code>) or a unique substring of the Site path.</p> <pre><code>$ balsam queue submit --site=123 # ...\n$ balsam queue submit --site=unique_name # ...\n</code></pre>"},{"location":"user-guide/elastic/","title":"Auto-scaling Resources with Balsam Elastic Queue","text":"<p>A Balsam Site does not automatically use your computing allocation by default.  Instead, we must use the BatchJob API or <code>balsam queue</code> CLI to explicitly request compute nodes at a given Balsam Site.  This follows the principle of least surprise: as the User, you explicitly decide when and how resources are spent. This is not a limitation since we can remotely submit <code>BatchJobs</code> to any Site from a computer where Balsam is installed!</p> <p>However, we can opt-in to enable the <code>elastic_queue</code> plugin that creates <code>BatchJob</code> submissions on our behalf.  This can be a useful service in bursty or real-time workloads: instead of micro-managing the queues, we simply submit a stream of <code>Jobs</code> and allow the Site to provision resources as needed over time.</p>"},{"location":"user-guide/elastic/#enabling-the-elastic-queue-plugin","title":"Enabling the Elastic Queue Plugin","text":"<p>Auto-scaling is enabled at a Site by setting the <code>elastic_queue</code> configuration appropriately inside of the <code>settings.yml</code> file.</p> <p>You should find this line uncommented by default:</p> <pre><code>elastic_queue: null\n</code></pre> <p>Following this line is a commented-block showing an example <code>elastic_queue</code> configuration.  We want to comment out the <code>elastic_queue: null</code> line and uncomment the configuration; setting each of the parameters appropriately for our use case.</p> <p>Once the Plugin has been configured properly (see below), we must restart the Balsam Site Agent to load it:</p> <pre><code>$ balsam site sync\n\n# Or if the Site isn't already running:\n$ balsam site start\n</code></pre>"},{"location":"user-guide/elastic/#disabling-the-elastic-queue-plugin","title":"Disabling the Elastic Queue Plugin","text":"<p>To disable, comment out (or delete) the current <code>elastic_queue</code>  configuration in <code>settings.yml</code> and replace it with the line:</p> <pre><code>elastic_queue: null\n</code></pre> <p>Then restart the Balsam Site to run without the elastic queue plugin:</p> <pre><code>$ balsam site sync\n</code></pre>"},{"location":"user-guide/elastic/#configuring-the-elastic-queue","title":"Configuring the Elastic Queue","text":"<p>The configuration is fairly flexible to enable a wide range of use cases. This section explains the YAML configuration in chunks.</p>"},{"location":"user-guide/elastic/#project-queue-and-submit-frequency","title":"Project, Queue, and Submit Frequency","text":"<p>Firstly, <code>service_period</code> controls the waiting period (in seconds) between cycles in which a new <code>BatchJob</code> might be submitted to the queue.  The <code>submit_project</code>, <code>submit_queue</code>, and <code>job_mode</code> are directly passed through to the new <code>BatchJob</code>.  </p> <p>The <code>max_queue_wait_time_min</code> determines how long a submitted <code>BatchJob</code> should be enqueued before the elastic queue deletes it and tries re-submitting.  When using backfill to grab idle nodes (see next section), it makes sense to set a relatively short waiting time of 5-10 minutes.  Otherwise, this duration should be increased to a reasonable upper threshold to avoid deleting <code>BatchJobs</code> that have accrued priority in the queues.</p> <p>The elastic queue will maintain up to <code>max_queued_jobs</code> in the queue at any given time. This should be set to the maximum desired (or allowed) number of simultaneously queued/running <code>BatchJobs</code> at the Site. </p> <pre><code>elastic_queue:\n     service_period: 60\n     submit_project: \"datascience\"\n     submit_queue: \"balsam\"\n     job_mode: \"mpi\"\n     use_backfill: True\n     min_wall_time_min: 35\n     max_wall_time_min: 360\n     wall_time_pad_min: 5\n     min_num_nodes:  20\n     max_num_nodes: 127\n     max_queue_wait_time_min: 10\n     max_queued_jobs: 20\n</code></pre>"},{"location":"user-guide/elastic/#wall-time-and-backfilling","title":"Wall Time and Backfilling","text":"<p>Many HPC systems use backfilling schedulers, which attempt to place small Jobs while draining nodes for larger Jobs to start up at a determined future time. By opportunistically sizing jobs to fit into these idle node-hour windows, Balsam effectively \"fills the gaps\" in unused resources.  We enable this dynamic sizing with <code>use_backfill: True</code>.</p> <p>The interpretation of <code>min_wall_time_min</code> and <code>max_wall_time_min</code> depends on whether or not <code>use_backfill</code> is enabled:  </p> <ul> <li>When <code>use_backfill</code> is <code>False</code>: <code>min_wall_time_min</code> is ignored and BatchJobs are submitted for a constant wallclock time limit of <code>max_wall_time_min</code>.</li> <li>When <code>use_backfill</code> is <code>True</code>: Balsam selects backfill windows that are at least as long as <code>min_wall_time_min</code> (this is to avoid futile 5 minute submissions when all Jobs take at least 30 minutes). The wallclock time limit is then the lesser of the scheduler's backfill duration and <code>max_wall_time_min</code>.</li> <li>Finally, a \"padding\" value of <code>wall_time_pad_min</code> is subtracted from the    final wallclock time in all <code>BatchJob</code> submissions.  This should be set to a couple minutes when <code>use_backfill</code> is <code>True</code> and <code>0</code> otherwise.</li> </ul> <pre><code>elastic_queue:\n     service_period: 60\n     submit_project: \"datascience\"\n     submit_queue: \"balsam\"\n     job_mode: \"mpi\"\n     use_backfill: True\n     min_wall_time_min: 35\n     max_wall_time_min: 360\n     wall_time_pad_min: 5\n     min_num_nodes:  20\n     max_num_nodes: 127\n     max_queue_wait_time_min: 10\n     max_queued_jobs: 20\n</code></pre>"},{"location":"user-guide/elastic/#node-count","title":"Node Count","text":"<p>Finally, the <code>min_num_nodes</code> and <code>max_num_nodes</code> determine the permissible range of node counts in submitted <code>BatchJobs</code>. When operating with the <code>use_backfill=True</code> constraint, backfill windows smaller than <code>min_num_nodes</code> will be ignored.  Otherwise, BatchJob submissions use <code>min_num_nodes</code> as a lower bound. Likewise, <code>max_num_nodes</code> gives an upper bound on the BatchJob's node count. </p> <p>The actual submitted BatchJob node count falls somewhere in this range.  It is determined from the difference between how many nodes are currently requested (queued or running BatchJobs) and the aggregate node footprint of all runnable Jobs.</p> <pre><code>elastic_queue:\n     service_period: 60\n     submit_project: \"datascience\"\n     submit_queue: \"balsam\"\n     job_mode: \"mpi\"\n     use_backfill: True\n     min_wall_time_min: 35\n     max_wall_time_min: 360\n     wall_time_pad_min: 5\n     min_num_nodes:  20\n     max_num_nodes: 127\n     max_queue_wait_time_min: 10\n     max_queued_jobs: 20\n</code></pre> <p>Therefore, the elastic queue automatically controls the size and number of requested BatchJobs as the workload grows.  We can think of each <code>BatchJob</code> as a flexibly-sized block of resources, and the elastic queue creates multiple blocks (one per <code>service_period</code>) while choosing their sizes.   If one BatchJob does not accommodate the incoming volume of tasks, then multiple BatchJobs of the maximum size are submitted at each iteration.</p> <p>When the incoming Jobs slow down and the backlog falls inside the <code>(min_num_nodes, max_num_nodes)</code> range, the <code>BatchJobs</code> reduce down to a single, smaller allocation of resources.  As utilization decreases and launchers become idle, the nodes are released according to the launcher's <code>idle_ttl_sec</code> configuration (also in <code>settings.yml</code>).</p>"},{"location":"user-guide/installation/","title":"Installation","text":"<p>Balsam requires Python3.7+ and is tested on Linux and MacOS. Within any suitable Python environment, Balsam can be installed using <code>pip</code>:</p> <pre><code># Use --pre to get the Balsam pre-release\n$ pip install --pre balsam\n</code></pre> <p>Balsam developers or service administrators should instead follow the developer installation instructions.</p>"},{"location":"user-guide/installation/#supported-sites","title":"Supported Sites","text":"<p>Balsam is easily extensible to new HPC systems. Default configurations are available for the following systems:</p> Facility System Configuration Included? ALCF Aurora ALCF Polaris ALCF Sunspot NERSC Perlmutter-CPU NERSC Perlmutter-GPU --- Mac OS"},{"location":"user-guide/jobs/","title":"Balsam Jobs","text":""},{"location":"user-guide/jobs/#key-concept-the-job-lifecycle","title":"Key Concept: The Job Lifecycle","text":"<p>The Balsam <code>Job</code> represents a single invocation of an <code>App</code> on some specified computing resources. Each <code>Job</code> is a stateful object that advances through a lifecycle of states (from <code>CREATED</code> to <code>JOB_FINISHED</code> in a successful flow).  </p> <p>After defining the requisite <code>Apps</code>, we create a collection of <code>Jobs</code>. Each Job specifies any data transfer or inter-job dependencies. The collection of <code>Jobs</code> represents our workflow, which is then executed by Balsam over time.</p> <p>Jobs do not automatically run!</p> <p>As mentioned in the quickstart, Jobs only specify the CPU/GPU resources needed for each task.  In order to run Jobs, we then request a block of node-hours by submitting a BatchJob.  This section goes into detail on managing <code>Jobs</code>, which is a separate concern in Balsam. You will find your jobs waiting in the <code>PREPROCESSED</code> state until a <code>BatchJob</code> begins running.</p> <p>In the normal (successful) flow of execution, a <code>Job</code> moves through the following sequence of states.  The table below defines each state as well as the  action performed by Balsam to move the job toward the next state.</p> State Meaning Next Balsam Action <code>CREATED</code> Job initially submitted. Check the parent Job and data transfer dependencies <code>AWAITING_PARENTS</code> Pending parent job dependencies. Advance to <code>READY</code> when all parents finished <code>READY</code> All parent jobs have finished. Submit any stage-in transfer tasks <code>STAGED_IN</code> All data dependencies staged in. Call the <code>preprocess()</code> hook <code>PREPROCESSED</code> The <code>preprocess</code> hook completed. Acquire and launch the executable on a compute node <code>RUNNING</code> Started executing on a compute node. Poll the executing Job process's return code <code>RUN_DONE</code> Execution finished with return <code>0</code>. Call the <code>postprocess()</code> hook <code>POSTPROCESSED</code> The <code>postprocess</code> hook completed. Submit any stage-out transfer tasks <code>STAGED_OUT</code> All stage-out transfers completed. Mark the job <code>JOB_FINISHED</code> <code>JOB_FINISHED</code> The job has completed processing. Nothing (end state) <p>Additionally, Balsam defines the following exceptional states for handling jobs that encounter errors or early termination:</p> State Meaning Next Balsam Action <code>RUN_ERROR</code> Execution finished with nonzero returncode. Call the <code>handle_error()</code> hook <code>RUN_TIMEOUT</code> Execution was terminated mid-run. Call the <code>handle_timeout()</code> hook <code>RESTART_READY</code> Job is ready to run again. Acquire and launch the executable on a compute node <code>FAILED</code> Completed processing (unsuccessfully). Nothing (end state) <p>Hopefully, it's clear from these state flows that a <code>Job</code> can be thought of as the workflow surrounding a single App run. You should check out specific examples of the Balsam hooks that can be used to build interesting workflows at the <code>ApplicationDefinition</code> level. If we don't define any special hooks or data transfers, most of the steps listed above are no-ops and the <code>Job</code> simplifies down to a simple run of an App command.</p> <p>Of course, we can also build DAGs or ensembles of many application runs by creating multiple <code>Jobs</code>,  potentially specifying inter-<code>Job</code> dependencies.  We will show effective methods for creating large batches of Jobs later on.  To conclude this section, the state diagram below summarizes the Job lifecycle by illustrating the common state flows.</p> stateDiagram-v2     created: Created     awaiting_parents: Awaiting Parents     ready: Ready     staged_in: Staged In     preprocessed: Preprocessed     restart_ready: Restart Ready     running: Running     run_done: Run Done     postprocessed: Postprocessed     staged_out: Staged Out     finished: Job Finished     run_error: Run Error     run_timeout: Run Timeout     failed: Failed      created --&gt; ready: No parents     created --&gt; awaiting_parents: Pending dependencies     awaiting_parents --&gt; ready: Dependencies finished     ready --&gt; staged_in: Transfer external data in     staged_in --&gt; preprocessed: Run preprocess script     preprocessed --&gt; running: Launch job      running --&gt; run_done: Return code 0     running --&gt; run_error: Nonzero return     running --&gt; run_timeout: Early termination      run_timeout --&gt; restart_ready: Auto retry     run_error --&gt; restart_ready: Run error handler     run_error --&gt; failed: No error handler     restart_ready --&gt; running: Launch job      run_done --&gt; postprocessed: Run postprocess script     postprocessed --&gt; staged_out: Transfer data out     staged_out --&gt; finished: Job Finished"},{"location":"user-guide/jobs/#creating-jobs","title":"Creating Jobs","text":"<p>To create a <code>Job</code>, we need to supply arguments via the Python API's <code>Job()</code> constructor or the <code>balsam job create</code> CLI.  Most fields are optional and take sensible default values. At a minimum, we must always supply:</p> <ul> <li><code>app_id</code> or <code>app_name</code> and <code>site_path</code>: reference to the specific <code>App</code></li> <li><code>workdir</code>: the working directory, relative to the Site's <code>data/</code> path</li> <li>Any <code>parameters</code> required by the <code>ApplicationDefinition</code>'s command template</li> <li>Any <code>transfers</code>  items required by the <code>ApplicationDefinition</code></li> </ul> <p>We can also create <code>Jobs</code> using the <code>ApplicationDefinition.submit()</code> shorthand: this removes the need for <code>app_id</code> because the value is inferred from the application class itself.</p>"},{"location":"user-guide/jobs/#cli-job-creation","title":"CLI Job Creation","text":"<p>The quickstart tutorial showed an example of CLI job creation:</p> <pre><code>$ balsam job create --site=laptop --app Hello --workdir demo/hello2 --param say_hello_to=\"world2\" \n</code></pre> <p>If <code>-a/--app</code> doesn't uniquely specify an App by its class path, you can provide the numeric app ID (revealed by <code>balsam app ls</code>) or target a specific site using the <code>--site</code> selector.  Since you can target any App defined at any Site, the process of submitting Jobs locally or between systems is seamless and unified.</p> <p>By passing <code>test/1</code> to the <code>-w/--workdir</code> option, we declare that the job should run in the <code>data/test/1/</code> subdirectory of the Site.  This folder will be created automatically.</p> <p>Finally, multiple command template parameters can be passed by repeated <code>-p/--param</code> arguments.  In the example above we have only one parameter called <code>name</code> and provide a value of <code>\"world\"</code>.</p> <p>Multiple Arguments</p> <p>Run <code>balsam job create --help</code> to list the various options and example usage.  For any option that takes multiple arguments, they should be provided by repeating the flag. For instance, <code>balsam job create --tag foo=xyz --tag experiment=initial</code> will create a job with two tags.</p>"},{"location":"user-guide/jobs/#api-job-creation","title":"API Job Creation","text":"<p>You will usually prefer to leverage the flexibility of Python to populate a large number of <code>Jobs</code> programmatically. For example, a common pattern in Balsam is to write a quick one-off script to crawl a directory of input files and generate a Job for each one.  Our entrypoint to creating <code>Jobs</code> from Python is the Balsam <code>Job</code> API:</p> <pre><code>from balsam.api import Job\n</code></pre> <p>Take advantage of the docstrings and type annotations!</p> <p>We strongly recommend using the Balsam APIs in an interactive development environment such as a Jupyter Notebook or Python IDE of choice.  Each Balsam model defined under <code>balsam.api</code> includes detailed docstrings and type annotations for the possible methods.</p> <p>We can construct an in-memory <code>Job</code> object by populating the required fields, and then we submit it to the web service by calling <code>job.save()</code>. This is the Python equivalent of the previous CLI example:</p> <pre><code>job = Job(app_id=123, workdir=\"test/1\", parameters={\"name\": \"world!\"})\njob.save()\n</code></pre> <p>If you don't want to lookup and hard-code the <code>app_id</code>, you can provide the app name in its place.  If you're using the same app name at multiple Sites, you will also have to provide the <code>site_name</code> to disambiguate which app you really mean to create:</p> <pre><code>job = Job(\n    app_id=\"Hello\",\n    site_name=\"polaris\",\n    workdir=\"test/1\",\n    parameters={\"name\": \"world!\"}\n)\njob.save()\n</code></pre> <p>A shortcut for creating and saving the <code>Job</code> in one step is provide the same exact arguments to <code>Job.objects.create</code>:</p> <pre><code>job = Job.objects.create(app_id=123, ...)\n# don't need to call job.save()\n</code></pre> <p>The real advantage of the API is to create many related <code>Jobs</code> programmatically.  We can still call <code>job.save()</code> one-by-one, but it's more efficient to bulk-create the jobs with a single network round trip:</p> <pre><code>jobs = [\n    Job(app_id=123, workdir=f\"test/{n}\", parameters={\"name\": f\"world {n}!\"})\n    for n in range(10)\n]\n\n# Capture `jobs` as return value!\njobs = Job.objects.bulk_create(jobs)\n</code></pre> <p><code>bulk_create</code> does not modify objects in place!</p> <p>When passing a list of Jobs into <code>bulk_create()</code>, you must use the returned value to overwrite the input list with the newly-created Jobs.  This is necessary to set the ID on each item as generated by the server.  Otherwise, the created Jobs will have <code>id == None</code> and generally behave like objects that have never been saved to the API.</p> <p>Finally, <code>ApplicationDefinitions</code> provide a convenient shorthand to create <code>Jobs</code> from the same file that the application is defined in:</p> <pre><code>job = Hello.submit(workdir=\"test/123\", name=\"world!\")\n</code></pre> <p>When using the <code>ApplicationDefinition.submit()</code> syntax, the <code>app_id</code> is automatically inferred, and any unrecognized keyword arguments are passed through into <code>job.parameters</code>. This allows for a very concise Job creation. To use <code>submit</code> with bulk-creation, pass <code>save=False</code> to avoid saving each <code>Job</code> to the API one a time: </p> <pre><code>jobs = [\n    Hello.submit(workdir=f\"test/{n}\", say_hello_to=f\"world {n}!\", save=False)\n    for n in range(10)\n]\njobs = Job.objects.bulk_create(jobs) # efficient creation\n</code></pre>"},{"location":"user-guide/jobs/#tagging-jobs","title":"Tagging Jobs","text":"<p>When creating many <code>Jobs</code> to run the same <code>App</code>, we need a way of keeping things organized and searchable.  Jobs should be organized into hierarchical working directories in a scheme that makes sense for your workflow.  Jobs can then be queried by working directory substrings, which facilitates monitoring groups *of Jobs having some common path fragment.</p> <p>However, organizing by <code>workdir</code> quickly becomes limited, so Balsam provides a more flexible system for tagging <code>Jobs</code> with arbitrary key-value string pairs.  You can assign <code>Jobs</code> any tag names and values (keeping in mind that even numerical tags are treated as strings), and then easily query or manipulate Jobs according to their tags.</p> <pre><code># Create tagged jobs...\n$ balsam job create --tag experiment=foo --tag system=H2O --tag run=5 # ...other args\n\n# ...so that you can fetch jobs with certain tags later!\n$ balsam job ls --tag experiment=foo\n</code></pre> <p>The idea is much the same with the Python API:</p> <pre><code>Job.objects.create(\n    # ...other kwargs here\n    tags={\"experiment\": \"foo\", \"system\": \"H2O\", \"run\": \"5\"}\n)\nfor job in Job.objects.filter(tags={\"experiment\": \"foo\"}):\n    if job.state == \"JOB_FINISHED\":\n        print(\"Finished:\", job.workdir)\n</code></pre>"},{"location":"user-guide/jobs/#associating-data-with-jobs","title":"Associating Data with Jobs","text":"<p>The Balsam service is not designed to store large volumes of data directly; instead, Balsam interfaces with external transfer providers such as Globus to orchestrate out-of-band data transfers efficiently.  Nevertheless, each Balsam <code>Job</code> contains a <code>data</code> attribute that can store a dictionary of arbitrary, JSON-serialized data.  This can be particularly useful to attach some user-defined, persistent state to <code>Jobs</code> that can be leveraged by the lifecycle hooks.</p> <pre><code># Creating Jobs with some initial data\nJob.objects.create(\n    # ...other kwargs here\n    data={\"retry_count\": 0, \"input_coords\": coords}\n)\n</code></pre> <p>Note that in order to update <code>job.data</code> on an existing Job, we need to assign a new dictionary to the <code>job.data</code> attribute, rather than setting an individual key:</p> <pre><code>job = Job.objects.get(tags={\"experiment\": \"foo\"}, workdir__contains=\"foo/20\")\ndat = job.data\nretry_count = dat[\"retry_count\"] + 1\n\n# Merge the old job.data with an incremented value:\njob.data = {**dat, \"retry_count\": retry_count + 1}\njob.save()\n</code></pre> <p>This is a consequence of the descriptor protocol used to track mutations to  <code>Job</code> fields. The Python API currently sends only fields which have been expclitly set (<code>job.FIELD = VALUE</code>) in updates to the backend. If you modify an existing mutable field (appending to a list or setting a new key on the <code>data</code> dictionary), the change cannot yet be detected by the Balsam client API layer.</p>"},{"location":"user-guide/jobs/#defining-compute-resources","title":"Defining Compute Resources","text":"<p>The default <code>Job</code> arguments assume the application will execute as one single-threaded process occupying a full compute node.  This is often not the case, and the <code>Job</code> constructor provides several options to specify precise resource requirements. As usual, these parameters can be specified via the Python API or CLI when creating new Jobs.  </p> <ul> <li><code>num_nodes</code>: number of compute nodes needed in a multi-node MPI application</li> <li><code>ranks_per_node</code>: number of processes (MPI ranks) per compute node</li> <li><code>threads_per_rank</code>: number of threads per rank</li> <li><code>threads_per_core</code>: number of threads per physical CPU core</li> <li><code>launch_params</code>: optional pass-through parameters to MPI launcher</li> <li><code>gpus_per_rank</code>: number of GPU accelerators per rank<sup>*</sup></li> <li><code>node_packing_count</code>: maximum number of <code>Jobs</code> that may run simultaneously on the same compute node</li> <li><code>wall_time_min</code>: optional Job execution time estimate, in minutes</li> </ul> <p>!!! note \"Aurora and Sunspot GPUS     On Aurora and Sunspot, Balsam considers a GPU tile to be a GPU for the purposes of setting <code>gpus_per_rank</code>.  Setting <code>gpus_per_rank=1</code> on Aurora and Sunspot will place one rank per tile.  To set one rank per full GPU, set <code>gpus_per_rank=2</code>.</p> <p>Balsam dynamically schedules <code>Jobs</code> onto the available compute resources over the course of each launcher (pilot batch job). Each <code>Job</code> is considered to fully occupy a whole number of CPU cores and GPU devices while it runs.  For each compute node in a batch allocation, Balsam tracks the list of busy CPUs, busy GPUs, and a node occupancy metric.  The occupancy is a floating-point value between 0.0 (idle) and 1.0 (busy) calculated as the sum of <code>1 / job.node_packing_count</code> over all <code>jobs</code> running on a node. When Balsam places a sub-node job, it simultaneously honors the constraints:</p> <ul> <li>The node must have enough idle GPUs (<code>job.ranks_per_node * job.gpus_per_rank</code>)</li> <li>The node must have enough idle CPUs (<code>job.ranks_per_node * job.threads_per_rank // job.threads_per_core</code>)</li> <li>The node must have low enough occupancy to accommodate the <code>job</code> without exceeding an occupancy of 1.0.</li> </ul> <p>Job Placement Examples</p> <p>Consider a 2-node allocation on a system with 64 CPU cores and 8 GPUs per node.  </p> <ul> <li>If there are 16 single-process <code>jobs</code> with <code>node_packing_count=8</code> and <code>gpus_per_rank=1</code>, then all 16 runs will execute concurrently.  </li> <li>With <code>node_packing_count=4</code> and <code>gpus_per_rank=1</code>, only 8 jobs will run at a time (4 per node, constrained by the node occupancy).</li> <li>If <code>node_packing_count=8</code> and <code>gpus_per_rank=8</code>, only 2 jobs will run at a time (one job per node, constrained by the lack of idle GPUs).</li> </ul>"},{"location":"user-guide/jobs/#parent-job-dependencies","title":"Parent Job Dependencies","text":"<p>By including parent Job IDs in a <code>Job</code> constructor, we create dependencies: every <code>Job</code> waits to begin executing until all of its parents reach the <code>JOB_FINISHED</code> state. This can be used to build workflows comprising several <code>Apps</code>. Moreover, since <code>Jobs</code> can be created in lifecycle hooks, we can leverage this ability to dynamically change the workflow graph as Jobs are executed.</p> <pre><code># Create a Job that depends on job1 and job2:\nJob.objects.create(\n    # ...other kwargs\n    parent_ids=[job1.id, job2.id]\n)\n</code></pre>"},{"location":"user-guide/jobs/#data-transfer","title":"Data Transfer","text":"<p>In the App Transfer Slots section, we explained how <code>ApplicationDefinition</code> classes define requirements for remote data stage in before execution (or stage out after execution).  This scheme has two advantages:</p> <ul> <li>The inputs and outputs for an <code>App</code> are explicitly defined and consistently named   alongside the other ingredients of the <code>ApplicationDefinition</code> class. This pattern   facilitates writing command templates and lifecycle hooks that are decoupled from   details like external file paths.</li> <li>The Balsam Site Agent automatically groups transfers from endpoint <code>A</code> to <code>B</code> across many Jobs.  It can then submit batched transfer tasks to the underlying transfer service provider, and those transfers are monitored until completion. As soon as input data arrives, waiting jobs transition from <code>READY</code> to <code>STAGED_IN</code> and begin preprocessing.  The end-to-end lifecycle (await parent dependencies, stage in datasets, preprocess, schedule compute nodes, launch application) is fully managed by the Site Agent.</li> </ul> <p>Since the <code>ApplicationDefinition</code> decides how files are named in local working directories, we only need to fill the Transfer Slots by providing remote data locations. The Job <code>transfers</code> argument is a dictionary of the form <code>{\"slot_name\": \"location_alias:absolute_path\"}</code>.</p> <ul> <li><code>slot_name</code> must match one of the App's transfer keys.</li> <li><code>location_alias</code> must match one of the keys in the <code>transfer_locations</code> dictionary in <code>settings.yml</code>. </li> <li><code>absolute_path</code> is the fully-resolved path to the source file or directory for stage-ins. For stage-outs, it is the path of the destination to be written.</li> </ul> <p>Adding Location Aliases</p> <p>The <code>transfer_locations</code> dictionary in <code>settings.yml</code> maps location aliases to values of the form <code>protocol://network_location</code>.  A useful example would be a Globus Connect Personal endpoint running on your laptop.  The corresponding list item under <code>transfer_locations</code> in <code>settings.yml</code> would look like this:</p> <pre><code>transfer_locations:\n    laptop: globus://9d6d99eb-6d04-11e5-ba46-22000b92c6ec\n</code></pre> <p>In this example, we create a <code>Job</code> that runs on a supercomputer but copies the <code>input_file</code> from our laptop and eventually writes the <code>result</code> back to it.</p> <pre><code>Job.objects.create(\n    # ...other kwargs\n    transfers={\n        # Using 'laptop' alias defined in settings.yml\n        \"input_file\": \"laptop:/path/to/input.dat\",\n        \"result\": \"laptop:/path/to/output.json\",\n    },\n)\n</code></pre>"},{"location":"user-guide/jobs/#querying-jobs","title":"Querying Jobs","text":"<p>Once many <code>Jobs</code> are added to Balsam, there are several effective ways of searching and manipulating those jobs from the CLI or Python API. The Balsam query API has a regular structure that's the same for all resources (<code>Site</code>, <code>App</code>, <code>Job</code>, <code>BatchJob</code>, etc...) and loosely based on the Django ORM.  Refer to the next section on the Balsam API for general details that apply to all resources.  In the following, we focus on <code>Job</code> specific examples, because those are the most common and useful queries you'll be performing with Balsam.  </p> <p>The CLI is just a wrapper of the API</p> <p>While the examples focus on the Python API, the CLI is merely a thin wrapper of the API, and most CLI queries can be inferred from the <code>balsam job ls --help</code> menu.</p> <p>For example, this Python query: <pre><code>Job.objects.filter(state=\"FAILED\", tags={\"experiment\": \"foo\"})\n</code></pre></p> <p>is equivalent to this command line: <pre><code>$ balsam job ls --state FAILED --tag experiment=foo\n</code></pre></p>"},{"location":"user-guide/jobs/#filtering-examples","title":"Filtering Examples","text":"<p><code>Job.objects</code> is a Manager class that talks to the underlying REST API over HTTPS and builds <code>Job</code> objects from JSON data transferred over the Web.  We can start to build a query with one or many filter kwargs passed to Job.objects.filter().  The <code>filter</code> docstrings are again very useful here in listing the supported query parameters within your IDE.  Queries are chainable and lazily-evaluated:</p> <pre><code>from balsam.api import Site, Job\n\n# This hits the network and immediately returns ONE Site object:\nmy_site = Site.objects.get(name=\"my-site\", path=\"my-site\")\n\n# This doesn't hit the network yet:\nfoo_jobs = Job.objects.filter(\n    site_id=my_site.id, \n    tags={\"experiment\": \"foo\"},\n)\n\n# We chain the query and iterate, triggering HTTPS request:\nfailed_workdirs = [\n    j.workdir \n    for j in foo_jobs.filter(state=\"FAILED\")\n]\n</code></pre> <p>We can generate a query that returns all Jobs across Balsam:</p> <pre><code>all_jobs = Job.objects.all()\n</code></pre> <p>Queries support slicing operations: <pre><code>some_jobs = all_jobs[5:15]\n</code></pre></p> <p>We can count the number of Jobs that satisfy some query: <pre><code>Job.objects.filter(state=\"RUNNING\").count()\n</code></pre></p> <p>We can order the Jobs according to some criterion (prefix with <code>-</code> for descending order): <pre><code>ten_most_recent_finished = Job.objects.filter(\n    state=\"JOB_FINISHED\"\n).order_by(\"-last_update\")[:10]\n</code></pre></p> <p>We can build up queries based on numerous criteria, including but not limited to:</p> <ul> <li><code>workdir__contains</code>: path fragment</li> <li><code>tags</code></li> <li><code>app_id</code></li> <li><code>state</code></li> <li><code>parameters</code></li> <li><code>id</code> (single or list of Job IDs)</li> <li><code>parent_id</code> (single or list of Parent Job IDs)</li> </ul> <p>The iterable queries return <code>Job</code> instances that can be inspected or modified and updated by calling <code>job.save()</code>.  Again, refer to the docstrings or use <code>help(Job)</code> to see a detailed listing of the Job fields.</p> <pre><code>failed = Job.objects.filter(workdir__contains=\"production-X\", state=\"FAILED\")\n\nfor job in failed:\n    if job.return_code == 1:\n        job.num_nodes = 16\n        job.state = \"RESTART_READY\"\n        job.save()\n</code></pre>"},{"location":"user-guide/jobs/#resolving-the-workdir","title":"Resolving the Workdir","text":"<p>The <code>Job.workdir</code> attribute is given and stored relative to the Site <code>data/</code> directory.  Sometimes it is useful to resolve an absolute path to a job's working directory:</p> <pre><code># From a given Site and Job...\nsite = Site.objects.get(id=123)\njob = Job.objects.get(id=456)\n\n# The workdir can be constructed with Path join operators:\nabs_workdir = site.path / \"data\" / job.workdir\n\n# Or with the helper method:\nabs_workdir = job.resolve_workdir(site.path / \"data\")\n</code></pre> <p>If you are running code from inside a Site, you can access the current Site configuration and the resolved <code>data/</code> path:</p> <pre><code>from balsam.api import site_config\n\nabs_workdir = job.resolve_workdir(site_config.data_path)\n</code></pre>"},{"location":"user-guide/jobs/#accessing-parents","title":"Accessing Parents","text":"<p><code>Jobs</code> contain the special method <code>parent_query()</code> which returns a iterable query over the job's parents.  For example, we can combine this with <code>resolve_workdir</code> to list the parents' working directories.  This pattern can be particularly useful in the preprocessing app hook, where a Job needs to read some data from its parents before executing:</p> <pre><code>from balsam.api import site_config\nfrom balsam.api import ApplicationDefinition\n\nclass MyApp(ApplicationDefinition):\n    # ...other class attrs\n\n    def preprocess(self):\n        parent_workdirs = [\n            j.resolve_workdir(site_config.data_path)\n            for j in self.job.parent_query()\n        ]\n</code></pre>"},{"location":"user-guide/jobs/#updating-jobs","title":"Updating Jobs","text":"<p>In addition to the examples where <code>Jobs</code> were modified and updated by calling <code>save()</code>, we can efficiently apply the same update to all jobs matching a particular query.  This can be significantly faster than calling <code>job.save()</code> in a loop, which repeatedly sends small HTTP update requests over the wire.</p> <pre><code># Run all the Failed Jobs again:\nJob.objects.filter(state=\"FAILED\").update(state=\"RESTART_READY\")\n</code></pre>"},{"location":"user-guide/jobs/#deleting-jobs","title":"Deleting Jobs","text":"<p>Just as we can apply updates to individual jobs or job sets selected by a query, we can also delete <code>Jobs</code>:</p> <pre><code># Delete a single job:\njob.delete()\n\n# Delete a collection of Jobs:\nJob.objects.filter(state=\"FAILED\", tags={\"run\": \"H2O\"}).delete()\n</code></pre>"},{"location":"user-guide/jobs/#python-app-futures","title":"Python App Futures","text":"<p>When using the <code>run() function</code> in your <code>ApplicationDefinitions</code>, you can treat the resultant <code>Job</code> objects like standard Python Futures in a few useful ways.</p>"},{"location":"user-guide/jobs/#accessing-results","title":"Accessing Results","text":"<p>The <code>Job.result(timeout=None)</code> method will block until the job is completed, and return the propagated return value of the <code>run()</code> function.  If the function raised an Exception, the Exception is re-raised:</p> <p><pre><code>job = Adder.submit(\"test/123\", x=3, y=7)\nassert job.result() == 3 + 7\n</code></pre> <code>result()</code> optionally takes a floating point seconds timeout value.  If the Job does not complete within the timeout period, it will raise <code>concurrent.futures.TimeoutError</code>.</p> <p>Similarly, <code>result_nowait()</code> will return the result in a non-blocking fashion and raise  <code>Job.NoResult</code> if a result is not immediately available.</p>"},{"location":"user-guide/jobs/#polling-a-single-job-on-completion","title":"Polling a single job on completion","text":"<p><code>Job.done()</code> polls the API and returns <code>True</code> if the Job is either in the <code>JOB_FINISHED</code> or <code>FAILED</code> state.</p>"},{"location":"user-guide/jobs/#polling-many-jobs-on-completion","title":"Polling many jobs on completion","text":"<p>The <code>Job.objects.wait()</code> function takes a list of <code>Jobs</code> and behaves otherwise analogously to <code>concurrent.futures.wait</code>.  This can be used to efficiently poll on a large collection of Jobs with a timeout  and sort the results by completed/in-progress Jobs:</p> <pre><code>wait_result = Job.objects.wait(\n    active_jobs, \n    return_when=\"FIRST_COMPLETED\",\n    timeout=60, \n    poll_interval=10\n)\nprint(f\"{len(wait_result.done)} jobs completed\")\nprint(f\"{len(wait_result.not_done)} active jobs\")\n</code></pre>"},{"location":"user-guide/jobs/#iterating-over-jobs-as-they-complete","title":"Iterating over Jobs as they complete","text":"<p>The <code>Job.objects.as_completed()</code> function behaves analogously to <code>concurrent.futures.as_completed</code>.  The method returns an generator over the input <code>Jobs</code>, which yields <code>Jobs</code> one a time as they are completed.</p> <pre><code>for job in Job.objects.as_completed(active_jobs, timeout=60):\n    print(f\"Job {job.workdir} returned:  {job.result()}\")\n</code></pre>"},{"location":"user-guide/monitoring/","title":"Debugging, Monitoring, and Analytics","text":""},{"location":"user-guide/monitoring/#methods","title":"Methods","text":"<p>There are a few levels at which we can follow the workflows running in Balsam, diagnose errors, and gain insight into recent progress or efficiency. We can  always use the Python API or CLI to perform flexible queries:</p> <ul> <li>tracking Job states with <code>balsam job ls</code></li> <li>tracking BatchJob states with <code>balsam queue ls</code></li> </ul> <p>We can also simply read the files generated at the Balsam site.  Job-level standard output and error streams will appear in the <code>job.out</code> file of each Job's working  directory.  We can also search the Balsam logs for informative messages (e.g. when a Job is launched or an uncaught exception is thrown):</p> <ul> <li>reading the output of Jobs in the <code>data/</code> directory</li> <li>reading pre-/post-processing logs generated by the Balsam site agent (<code>logs/service*.log</code>)</li> <li>reading Job execution logs generated by the Balsam launcher   (<code>logs/mpi_mode*.log</code> or <code>logs/serial_mode*.log</code> for the respective job modes)</li> </ul> <p>Finally, Balsam provides an <code>EventLog</code> API that we can use to flexibly query Job events (state transitions) and visualize metrics like throughput or utilization within a particular subset of Jobs or BatchJobs:</p> <ul> <li>querying recent events with <code>EventLog</code></li> <li>generating visual reports using <code>balsam.analytics</code></li> </ul>"},{"location":"user-guide/monitoring/#the-eventlog-api","title":"The EventLog API","text":"<p>The timestamp of each Job state transition (e.g. <code>PREPROCESSED</code> --&gt; <code>RUNNING</code>) is recorded in the Balsam <code>EventLog</code> API.  We can leverage standard Balsam API queries to obtain a relevant set of Events to answer many interesting questions.</p> <p>Here, we print all the event details for each Job tagged with  <code>experiment=\"foo\"</code>: <pre><code>from balsam.api import EventLog\n\nfor evt in EventLog.objects.filter(tags={\"experiment\": \"foo\"}):\n    print(evt.job_id)      # Job ID\n    print(evt.timestamp)   # Time of state change (UTC)\n    print(evt.from_state)  # From which state the job transitioned\n    print(evt.to_state)    # To which state\n    print(evt.data)        # optional payload\n</code></pre></p>"},{"location":"user-guide/monitoring/#example-queries","title":"Example Queries","text":"<p>Refer to the <code>EventLog.objects.filter</code> docstrings for a comprehensive listing of the possible query parameters.  Here, we list just a few of the most useful queries.</p> <p>Get all <code>EventLogs</code> for a particular set of Job IDs: <pre><code>EventLog.objects.filter(job_id=[123, 124, 125])\n</code></pre></p> <p>Get <code>EventLogs</code> for all the Jobs that ran in a particular <code>BatchJob</code>:</p> <pre><code>EventLog.objects.filter(batch_job_id=123) # Using Balsam's intrinsic BatchJob ID (revealed by balsam queue ls --history)\nEventLog.objects.filter(scheduler_id=456) # Using the HPC scheduler's own ID (e.g. Cobalt ID)\n</code></pre> <p>Get <code>EventLogs</code> for all the Jobs having a set of matching tags: <pre><code>EventLog.objects.filter(tags={\"experiment\": \"ffn-1\", \"scale_nodes\": \"512\"}) \n</code></pre></p> <p>Get <code>EventLogs</code> for a particular state transition: <pre><code>EventLog.objects.filter(to_state=\"RUNNING\") # all Job startup events\nEventLog.objects.filter(from_state=\"RESTART_READY\", to_state=\"RUNNING\") # Only *restart* Events\n</code></pre></p> <p>Get <code>EventLogs</code> that occured within a certain UTC time range: <pre><code>from datetime import datetime, timedelta\n\nyesterday = datetime.utcnow() - timedelta(days=1)\nEventLog.objects.filter(timestamp_after=yesterday) # Only events more recent than 1 day\n</code></pre></p>"},{"location":"user-guide/monitoring/#the-analytics-api","title":"The Analytics API","text":"<p>We can certainly process the <code>EventLog</code> queries above however we want (e.g. to count how many Jobs started in a certain experiment or a certain time interval). However, the <code>balsam.analytics</code> provides convenience methods for the most common Balsam Event processing tasks.</p> <p>In particular, we can use this module to visualize:</p> <ul> <li>Throughput:  the finished Job count as a function of time</li> <li>Utilization: how many Jobs were running as a function of time</li> <li>Node availability: how many compute nodes were actively running a Balsam launcher as a function of time</li> </ul> <p>To perform these analyses, we simply call one of the following methods to generate time-series data:</p> <ul> <li><code>throughput_report(eventlog_query, to_state=\"JOB_FINISHED\")</code></li> <li><code>utilization_report(eventlog_query, node_weighting=True)</code></li> <li><code>available_nodes(batchjob_query)</code></li> </ul> <p>Each of these methods returns a 2-tuple of <code>(X, Y)</code> data that fits seamlessly within a <code>matplotlib</code> visualization workflow.  We show some specific examples below.</p>"},{"location":"user-guide/monitoring/#throughput","title":"Throughput","text":"<p>In this example, we visualize the Job throughput versus elapsed minutes in one particular launcher run:</p> <pre><code>from balsam.api import EventLog\nfrom balsam.analytics import throughput_report\nfrom matplotlib import pyplot as plt\n\nevents = EventLog.objects.filter(scheduler_id=123)\n\ntimes, done_counts = throughput_report(events, to_state=\"RUN_DONE\")\n\nt0 = min(times)\nelapsed_minutes = [(t - t0).total_seconds() / 60 for t in times]\nplt.step(elapsed_minutes, done_counts, where=\"post\")\n</code></pre>"},{"location":"user-guide/monitoring/#utilization","title":"Utilization","text":"<p>We can look at how many nodes were actively running a Job at any given time using the same <code>EventLog</code> query from above. In this example, keeping the default <code>node_weighting=True</code> kwarg ensures that each Job is weighted by its resource requirements (so that 8 simultaneous jobs with <code>node_packing_count=8</code> contribute <code>8/8=1.0</code> to the overall utilization):</p> <pre><code>from balsam.api import EventLog\nfrom balsam.analytics import utilization_report\nfrom matplotlib import pyplot as plt\n\nevents = EventLog.objects.filter(scheduler_id=123)\n\ntimes, util = utilization_report(events, node_weighting=True)\n\nt0 = min(times)\nelapsed_minutes = [(t - t0).total_seconds() / 60 for t in times]\nplt.step(elapsed_minutes, util, where=\"post\")\n</code></pre>"},{"location":"user-guide/monitoring/#available-nodes","title":"Available Nodes","text":"<p>It's often most interesting to super-impose the utilization with the total number of available compute nodes.  For one <code>BatchJob</code> this is really easy: we just plot a horizontal line at the node count. If we are using auto scaling or have several BatchJob allocations overlapping in time, the available node count becomes a more complex step function in time.  We can generate this from a BatchJob query in Balsam:</p> <pre><code>from balsam.api import BatchJob\nfrom balsam.analytics import available_nodes\n\nfoo_batchjobs = BatchJob.objects.filter(\n    filter_tags={\"experiment\": \"foo\"}\n)\ntimes, node_counts = available_nodes(foo_batchjobs)\n</code></pre> <p>By overlaying this timeline with the Job utilization from above, we get an intuitive visual representation of how efficiently Balsam is using the available resources.  An example is shown below, where the thick, gray trace shows the <code>available_nodes</code>, while the thin blue trace shows the <code>utilization_report</code>. </p> <p></p>"},{"location":"user-guide/site-config/","title":"Balsam Sites","text":""},{"location":"user-guide/site-config/#sites-have-a-single-owner","title":"Sites have a single owner","text":"<p>All Balsam workflows are namespaced under Sites, which are self-contained project directories managed by an autonomous user agent. There is no concept of sharing Balsam Sites with other users: each Site has exactly one owner, who is the sole user able to see the Site. Therefore, to create a new Site, you must be authenticated with Balsam:</p> <pre><code>$ balsam login\n</code></pre> <p>Public logins temporarily are restricted</p> <p>The central Balsam service is currently in a pre-release phase and login is limited to pre-authorized ALCF users. For early access to Balsam, please send a request to the ALCF Help Desk.</p>"},{"location":"user-guide/site-config/#creating-a-site","title":"Creating a site","text":"<p>To initialize a Balsam site, use the CLI to select an appropriate default configuration for the current system.  Balsam creates a new Site directory and registers it with the REST API.  In order to use the Site, we must also start the agent process with <code>balsam site start</code>.</p> <pre><code>$ balsam site init SITE-PATH\n$ cd SITE-PATH\n$ balsam site start\n</code></pre> <p>The Site is populated with several folders and a bootstrapped configuration file <code>settings.yml</code>. The name that you choose for the Site must be unique across all of your Sites.  This name is used to identify and target Jobs to specific Sites.</p> <p></p> <p>You may need to restart the Site when the system is rebooted or otherwise goes down for maintenance.  You can always stop and restart the Site yourself:</p> <pre><code>$ balsam site stop\n$ balsam site start\n</code></pre>"},{"location":"user-guide/site-config/#the-site-directory","title":"The Site directory","text":"<p>Each Balsam site has a regular structure comprising certain files and directories:</p> <ul> <li><code>data/</code>:  Each Balsam Job runs in a subdirectory of <code>data/</code>.   The Job working directories   are specified relative to this folder via <code>job.workdir</code>.</li> <li><code>log/</code>:   The Site user agent and launcher pilot jobs send diagnostic messages here. Checking the logs is a great way to track exactly what is happening or what went wrong.</li> <li><code>qsubmit/</code>:  Balsam runs your apps inside of launchers which are submitted to the HPC batch scheduler via a shell script.  This directory contains each of the materialized scripts that was actually submitted to the batch queue.</li> <li><code>job-template.sh</code>:  This is the template for the  <code>qsubmit/</code>  scripts submitted to the HPC batch scheduler.</li> <li><code>settings.yml</code>:  This is where the Balsam Site is configured.  The file is populated with sensible defaults for the chosen platform, and it is commented for you to read and modify.</li> </ul>"},{"location":"user-guide/site-config/#customizing-the-job-template","title":"Customizing the Job Template","text":"<p>You can adapt the <code>job-template.sh</code> file to add custom scheduler flags, or to run code on your allocation before the pilot job starts.  Some examples of job template customization include:</p> <ul> <li>Adding scheduler-specific directives to the header of the shell script</li> <li>Configuring hardware (e.g. setting up MIG mode on GPUs)</li> <li>Staging data by copying files to node-local SSD's</li> <li>Loading modules or exporting global environment variables</li> </ul> <p>Feel free to add logic to the <code>job-template.sh</code> file as needed.  You can also maintain templates with different filenames, and point to the currently active job template by changing the <code>job_template_path</code> parameter in <code>settings.yml</code>.</p> <p>Keep Template Variables Intact</p> <p>Any values enclosed in double-curly braces (<code>{{launcher_cmd}}</code>) are variables provided to the template from Balsam.  Be sure to leave these names intact when modifying the template!</p> <p>Whenever you change the job template or Site settings file, it is necessary to  reload the Site if it's already running:</p> <pre><code>$ balsam site sync\n</code></pre> <p>Warning</p> <p>The <code>job-template.sh</code> and <code>settings.yml</code> are first loaded when the Site starts and stay in memory!  Therefore, any changes will not apply until you stop and restart the Site, using <code>balsam site stop</code> and <code>balsam site start</code> or <code>balsam site sync</code>.</p>"},{"location":"user-guide/site-config/#optional-template-variables","title":"Optional Template Variables","text":"<p>Because templates are generated with Jinja2, you can write custom templates leveraging variables, if-statements, for-loops, etc... referring to the Jinja2 template syntax.  You can define optional parameters exposed to the template by using the <code>optional_batch_job_params</code> key of <code>settings.yml</code>. This expects a dictionary mapping parameter names to default string values. </p> <p>These \"pass-through\" parameters can then be provided as extras on the command line via <code>-x/--extra-param</code> flags.</p>"},{"location":"user-guide/site-config/#customizing-the-settings","title":"Customizing the Settings","text":"<p>There are numerous adjustable parameters in <code>settings.yml</code> that control how the Site runs and processes your workflows.  The default values are designed to work on the chosen platform as-is, but users are encouraged to read the comments and modify <code>settings.yml</code> to suit their own needs.  </p> <p>Note</p> <p>Be sure to run <code>balsam site sync</code> to apply any changes to the Site agent!</p> <p>We highlight just a few of the important settings you may want to adjust:</p> <ul> <li><code>logging.level</code>: Change the verbosity to get more or less diagnostics from Balsam  in your <code>log/</code> directory.</li> <li><code>launcher.idle_ttl_sec</code>:  controls how long the pilot job should stay alive before quitting when nothing is running.  You might turn this up if you are debugging and want to hold on to resources.</li> <li><code>scheduler.allowed_projects</code>: lists the projects/allocations that the Site may submit to.  You need to update this to manage what allocations the Site may use.</li> <li><code>scheduler.allowed_queues</code>: defines the queueing policies per-queue name.  If a special reservation or partition is created for your project or a workshop,  you will need to define that here.</li> <li><code>processing.num_workers</code>: controls the number of simultaneous processes handling your Jobs' pre/post-processing workload.  If I/O-intensive preprocessing is a bottleneck, you can turn this value up.</li> <li><code>transfers.transfer_locations</code>: lets Balsam know about remote Globus endpoints     that the Site may stage data in/out from</li> <li><code>elastic_queue</code>: controls automated queue submissions by defining the granularity and flexibility of resource requests.  This is disabled by default and must be configured on an as-needed basis (see the Auto Scaling page for more information).</li> </ul>"},{"location":"user-guide/site-config/#the-site-cli","title":"The Site CLI","text":""},{"location":"user-guide/site-config/#starting-stopping-and-restarting-sites","title":"Starting, Stopping, and Restarting Sites","text":"<p>In order for workflows to actually run at a Site, the agent must be started as a background process on a login (or gateway) node.</p> <pre><code># To start and stop the Site agent:\n$ balsam site start\n$ balsam site stop\n\n# Restart the Site agent and push settings changes to the API:\n$ balsam site sync\n</code></pre> <p>Site Agent Resources</p> <p>The Balsam site agent runs as persistent daemon.  It can be started on any node with Internet access, access to the parallel filesystems, and access to the HPC resource manager.  This is typically a \"login\" or \"gateway\" node in a multi-user environment.</p> <p>The site agent runs a collection of plug-in modules responsible for various facets of the workflow.  These plug-ins can be configured, enabled, and disabled in the <code>settings.yml</code> file by adjusting data under these keys:</p> <ul> <li><code>scheduler</code>: interfaces with the HPC resource manager to submit and query batch resource allocations</li> <li><code>processing</code>: runs pre- and post- job execution lifecycle hooks to advance the workflow.</li> <li><code>transfers</code>: manages batch transfer tasks for stage in and stage out of job data</li> <li><code>elastic_queue</code>: automates batch job submissions to auto-scale resources to the runnable backlog</li> <li><code>file_cleaner</code>: clears unused data from working directories of finished jobs</li> </ul> <p>Launchers will still run if the agent is stopped</p> <p>Once submitted to the HPC queue, the Balsam launchers (pilot jobs) operate on the compute nodes independently of the Site agent.  As long as they have a valid access token, they will work regardless of whether the Site is running.  However, you typically want the Site to continue running so that new Jobs can be preprocessed for execution.</p>"},{"location":"user-guide/site-config/#listing-sites","title":"Listing Sites","text":"<p>The CLI is useful to get a quick listing of all the Sites you own: <pre><code># Summary of Sites:\n$ balsam site ls\n</code></pre></p> <p>The <code>Active</code> column indicates whether Sites have recently communicated with the API and are most likely up and running.  You can obtain much more detailed Site information (such as a listing of currently idle backfill windows at each Site) by including the <code>-v/--verbose</code> flag:</p> <pre><code># Detailed Site data:\n$ balsam site ls -v\n</code></pre>"},{"location":"user-guide/site-config/#moving-or-deleting-sites","title":"Moving or Deleting Sites","text":"<p>To permanently delete a Site and all associated workflows inside:</p> <pre><code>$ balsam site rm SITE-PATH\n</code></pre> <p>To rename a site or move it to a new directory:</p> <pre><code>$ balsam site mv SITE-PATH DESTINATION\n</code></pre>"},{"location":"user-guide/site-config/#why-is-there-a-delay-in-queue-submission","title":"Why is there a delay in queue submission?","text":"<p>Any Balsam CLI or Python API interaction (like running <code>balsam queue submit</code>), does not affect the HPC system directly. In fact, you're only manipulating resources in the REST API (creating a <code>BatchJob</code>). Eventually, the Site agent that runs in the background fetches state from the backend and performs local actions (run <code>sbatch</code> or <code>qsub</code>) to synchronize your Site with the central state.  </p> <p>This decoupled design, with all control flow routed through the central REST API, makes Balsam interactions completely uniform, whether you're running commands locally or remotely. For instance, you can provision resources on a remote supercomputer simply by specifying the <code>--site</code> on the command line:</p> <pre><code>$ balsam queue submit --site=my-site -n 1 -t 15 -q debug -A Project -j mpi\n</code></pre> sequenceDiagram     User-&gt;&gt;API:  `balsam queue submit`     API--&gt;&gt;User:  OK     Site-&gt;&gt;API:  Have any new BatchJobs for me?     API--&gt;&gt;Site:  Yes, here is one     Site-&gt;&gt;Cobalt:  `qsub` this BatchJob     Cobalt--&gt;&gt;Site:  New Cobalt Job ID     Site-&gt;&gt;API:  Update BatchJob with Cobalt Job ID     API--&gt;&gt;Site:  OK     User-&gt;&gt;API:  `balsam queue ls`     API--&gt;&gt;User:  Updated BatchJob"},{"location":"user-guide/transfer/","title":"Data Transfers","text":""},{"location":"user-guide/transfer/#background","title":"Background","text":"<p>Each Balsam <code>Job</code> may require data to be staged in prior to execution or staged out after execution. A core feature of Balsam is to interface with services such as Globus Transfer and automatically submit and monitor batched transfer tasks between endpoints.  </p> <p>This enables distributed workflows where large numbers of <code>Jobs</code> with relatively small datasets are submitted in real-time: the Site manages the details of efficient batch transfers and marks individual jobs as <code>STAGED_IN</code> as the requisite data arrives.</p> <p>To use this functionality, the first step is to define the Transfer Slots for a given Balsam App.  We can then submit <code>Jobs</code> with transfer items that fill the required transfer slots.  </p> <p>Be sure to read these two sections in the user guide for more information. The only other requirement is to configure the <code>transfer</code> plugin at the Balsam Site and authenticate with Globus, which we explain below.</p>"},{"location":"user-guide/transfer/#configuring-transfers","title":"Configuring Transfers","text":"<p>When using the Globus transfer interface, Balsam needs an access token to communicate with the Globus Transfer API.  You may already have an access token stored from a Globus CLI installation on your machine: check <code>globus whoami</code> to see if this is the case.  Otherwise, Balsam ships with the necessary tooling and you can follow the same Globus authentication flow by running:</p> <pre><code>$ balsam site globus-login\n</code></pre> <p>Next, we configure the <code>transfers</code> section of <code>settings.yml</code>:</p> <ul> <li><code>transfer_locations</code> should be set to a dictionary of trusted location aliases. If you need to add Globus endpoints, they can be inserted here. </li> <li><code>globus_endpoint_id</code> should refer to the endpoint ID of the local Site. </li> <li><code>globus_endpoint_site_path</code> specifies the path on the Globus endpoint, which might be different from the path used on login/compute nodes (e.g. for ALCF home filesystem, paths begin with /home/${USER}, but on the dtn_home endpoint, paths begin with /${USER}.)</li> <li><code>max_concurrent_transfers</code> determines the maximum number of in-flight transfer tasks, where each task manages a batch of files for many Jobs.</li> <li><code>transfer_batch_size</code> determines the maximum number of transfer items per transfer task.  This should be tuned depending on your workload (a higher number makes sense to utilize available bandwidth for smaller files).</li> <li><code>num_items_query_limit</code> determines the maximum number of transfer items considered in any single transfer task submission.</li> <li><code>service_period</code> determines the interval (in seconds) between transfer task submissions.</li> </ul> <p>Globus requires that you give Balsam consent to make transfers on your behalf; consent is granted for each endpoint that you intend to use. You can review your Globus consents here. For any endpoints that you have configured above (including the <code>globus_endpoint_id</code>), determine the Globus endpoint id, and execute the following command:</p> <p>balsam site globus-login -e ENDPOINT_ID1 -e ENDPOINT_ID2</p> <p>Note that <code>globus_endpoint_id</code> in settings.yaml will be used to stage input data in, and to stage output data out. This endpoint id will depend on the filesystem where your site is located (e.g. at ALCF, if it's in your home directory, use alcf#dtn_home; if it's on the Eagle filesystem, use alcf#eagle_dtn). Also make sure that the path to your site is set to correspond to how it is mapped on your Globus endpoint, using the <code>globus_endpoint_site_path</code> setting above.</p> <p>Once <code>settings.yml</code> has been configured appropriately, be sure to restart the Balsam Site:</p> <pre><code>$ balsam site sync\n</code></pre> <p>The Site will start issuing stage in and stage out tasks immediately and advancing Jobs as needed.  The state of transfers can be tracked using the Python API:</p> <pre><code>from balsam.api import TransferItem\n\nfor item in TransferItem.objects.filter(direction=\"in\",  state=\"active\"):\n    print(f\"File {item.remote_path} is currently staging in via task ID: {item.task_id}\")\n</code></pre>"}]}